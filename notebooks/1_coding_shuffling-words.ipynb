{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9974afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c7ddbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_path = \"../\"\n",
    "code_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cc3cac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed for pearson_correlation function.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(code_path)\n",
    "\n",
    "from encoding import *\n",
    "from preprocessing import get_ordered_representations, normalize_train_test, concat_past_features, lanczosinterp2D, delete_block_edges, shuffle_words\n",
    "from encoding import nested_blocked_cv, ridge_regression_fit_sklearn, ridge_regression_predict_torch\n",
    "from analysis import pearson_correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8c369f",
   "metadata": {},
   "source": [
    "## 0. load fmri data and annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43058c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_prefix = \"../data/HP_data/fMRI\"\n",
    "HF_home = \"/SWS/llms/nobackup/\"\n",
    "results_path_prefix = \"../results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71e0b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw fmri data for one subject\n",
    "fmri_data = np.load(f\"{data_path_prefix}/data_subject_I.npy\", allow_pickle=True)\n",
    "# fmri_data.shape, fmri_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3653a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing of each fmri TRs in seconds\n",
    "fmri_time = np.load(f\"{data_path_prefix}/time_fmri.npy\", allow_pickle=True)\n",
    "# fmri_time.shape, fmri_time[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cd0705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices of which TRs belong to which run.\n",
    "fmri_runs = np.load(f\"{data_path_prefix}/runs_fmri.npy\", allow_pickle=True)\n",
    "# fmri_runs.shape, fmri_runs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5a8f787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of words shown as stimuli (sequentially on a screen word by word)\n",
    "stimuli_words = np.load(f\"{data_path_prefix}/words_fmri.npy\", allow_pickle=True)\n",
    "# stimuli_words.shape, stimuli_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8946f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing in seconds of the words\n",
    "word_times = np.load(f\"{data_path_prefix}/time_words_fmri.npy\", allow_pickle=True)\n",
    "# word_times.shape, word_times[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c83b992",
   "metadata": {},
   "source": [
    "## 1. prepare the input sequence for the LLM\n",
    "(e.g. by changing formatting). Here we change \"+\" to \"\\n\\n\" and \"@\" to nothing (used to highlight italics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aeb6b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_input_sequence = []\n",
    "for word in stimuli_words:\n",
    "    LLM_input_sequence.append(word) if word != \"+\" and not \"@\" in word \\\n",
    "    else LLM_input_sequence.append(word.replace(\"+\", \"\\n\\n\").replace(\"@\",\"\"))\n",
    "assert len(word_times.tolist()) == len(LLM_input_sequence), \"different length\" # make sure we still have the same length as we have word timings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f4ab090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# from typing import List, Tuple\n",
    "\n",
    "# def shuffle_words(words: List[str], percentage: float, seed: int = 42, exclude_tokens: List[str] = [\"\\n\\n\"]) -> Tuple[List[str], List[int]]:\n",
    "\n",
    "#     assert 0 <= percentage <= 1, \"percentage must be between 0 and 1\"\n",
    "\n",
    "#     # Identify positions of words to consider for shuffling\n",
    "#     shuffle_indices = [i for i, w in enumerate(words) if w not in exclude_tokens]\n",
    "#     num_to_shuffle = int(len(shuffle_indices) * percentage)\n",
    "\n",
    "#     if num_to_shuffle == 0:\n",
    "#         return words.copy(), list(range(len(words)))\n",
    "\n",
    "#     # Randomly sample indices to shuffle\n",
    "#     random.seed(seed)\n",
    "#     indices_to_shuffle = random.sample(shuffle_indices, num_to_shuffle)\n",
    "\n",
    "#     # Extract and shuffle the selected words\n",
    "#     words_to_shuffle = [words[i] for i in indices_to_shuffle]\n",
    "#     random.shuffle(words_to_shuffle)\n",
    "\n",
    "#     # Insert shuffled words back\n",
    "#     shuffled_words = words.copy()\n",
    "#     for idx, new_word in zip(indices_to_shuffle, words_to_shuffle):\n",
    "#         shuffled_words[idx] = new_word\n",
    "\n",
    "#     # Build inverse mapping (from new positions back to original indices)\n",
    "#     inverse_indices = list(range(len(words)))\n",
    "#     for i, new_word in zip(indices_to_shuffle, words_to_shuffle):\n",
    "#         original_index = words.index(new_word)  # Caution: If duplicates exist, this gives first match\n",
    "#         inverse_indices[i] = original_index\n",
    "\n",
    "#     return shuffled_words, inverse_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41899c1d-343e-4136-9f99-c0bdf574dc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.str_('Harry'),\n",
       " np.str_('had'),\n",
       " np.str_('never'),\n",
       " np.str_('believed'),\n",
       " np.str_('he'),\n",
       " np.str_('would'),\n",
       " np.str_('meet'),\n",
       " np.str_('a'),\n",
       " np.str_('boy'),\n",
       " np.str_('he'),\n",
       " np.str_('hated'),\n",
       " np.str_('more'),\n",
       " np.str_('than'),\n",
       " np.str_('Dudley,'),\n",
       " np.str_('but'),\n",
       " np.str_('that'),\n",
       " np.str_('was'),\n",
       " np.str_('before'),\n",
       " np.str_('he'),\n",
       " np.str_('met'),\n",
       " np.str_('Draco'),\n",
       " np.str_('Malfoy.'),\n",
       " np.str_('Still,'),\n",
       " np.str_('first-year'),\n",
       " np.str_('Gryffindors'),\n",
       " np.str_('only'),\n",
       " np.str_('had'),\n",
       " np.str_('Potions'),\n",
       " np.str_('with'),\n",
       " np.str_('the'),\n",
       " np.str_('Slytherins,'),\n",
       " np.str_('so'),\n",
       " np.str_('they'),\n",
       " np.str_(\"didn't\"),\n",
       " np.str_('have'),\n",
       " np.str_('to'),\n",
       " np.str_('put'),\n",
       " np.str_('up'),\n",
       " np.str_('with'),\n",
       " np.str_('Malfoy'),\n",
       " np.str_('much.'),\n",
       " np.str_('Or'),\n",
       " np.str_('at'),\n",
       " np.str_('least,'),\n",
       " np.str_('they'),\n",
       " np.str_(\"didn't\"),\n",
       " np.str_('until'),\n",
       " np.str_('they'),\n",
       " np.str_('spotted'),\n",
       " np.str_('a'),\n",
       " np.str_('notice'),\n",
       " np.str_('pinned'),\n",
       " np.str_('up'),\n",
       " np.str_('in'),\n",
       " np.str_('the'),\n",
       " np.str_('Gryffindor'),\n",
       " np.str_('common'),\n",
       " np.str_('room'),\n",
       " np.str_('that'),\n",
       " np.str_('made'),\n",
       " np.str_('them'),\n",
       " np.str_('all'),\n",
       " np.str_('groan.'),\n",
       " np.str_('Flying'),\n",
       " np.str_('lessons'),\n",
       " np.str_('would'),\n",
       " np.str_('be'),\n",
       " np.str_('starting'),\n",
       " np.str_('on'),\n",
       " np.str_('Thursday'),\n",
       " np.str_('--'),\n",
       " np.str_('and'),\n",
       " np.str_('Gryffindor'),\n",
       " np.str_('and'),\n",
       " np.str_('Slytherin'),\n",
       " np.str_('would'),\n",
       " np.str_('be'),\n",
       " np.str_('learning'),\n",
       " np.str_('together.'),\n",
       " '\\n\\n',\n",
       " np.str_('\"Typical,\"'),\n",
       " np.str_('said'),\n",
       " np.str_('Harry'),\n",
       " np.str_('darkly.'),\n",
       " np.str_('\"Just'),\n",
       " np.str_('what'),\n",
       " np.str_('I'),\n",
       " np.str_('always'),\n",
       " np.str_('wanted.'),\n",
       " np.str_('To'),\n",
       " np.str_('make'),\n",
       " np.str_('a'),\n",
       " np.str_('fool'),\n",
       " np.str_('of'),\n",
       " np.str_('myself'),\n",
       " np.str_('on'),\n",
       " np.str_('a'),\n",
       " np.str_('broomstick'),\n",
       " np.str_('in'),\n",
       " np.str_('front'),\n",
       " np.str_('of'),\n",
       " np.str_('Malfoy.\"'),\n",
       " '\\n\\n',\n",
       " np.str_('He'),\n",
       " np.str_('had'),\n",
       " np.str_('been'),\n",
       " np.str_('looking'),\n",
       " np.str_('forward'),\n",
       " np.str_('to'),\n",
       " np.str_('learning'),\n",
       " np.str_('to'),\n",
       " np.str_('fly'),\n",
       " np.str_('more'),\n",
       " np.str_('than'),\n",
       " np.str_('anything'),\n",
       " np.str_('else.'),\n",
       " np.str_('\"You'),\n",
       " np.str_(\"don't\"),\n",
       " np.str_('know'),\n",
       " np.str_('that'),\n",
       " np.str_(\"you'll\"),\n",
       " np.str_('make'),\n",
       " np.str_('a'),\n",
       " np.str_('fool'),\n",
       " np.str_('of'),\n",
       " np.str_('yourself,\"'),\n",
       " np.str_('said'),\n",
       " np.str_('Ron'),\n",
       " np.str_('reasonably.'),\n",
       " np.str_('\"Anyway,'),\n",
       " np.str_('I'),\n",
       " np.str_('know'),\n",
       " np.str_(\"Malfoy's\"),\n",
       " np.str_('always'),\n",
       " np.str_('going'),\n",
       " np.str_('on'),\n",
       " np.str_('about'),\n",
       " np.str_('how'),\n",
       " np.str_('good'),\n",
       " np.str_('he'),\n",
       " np.str_('is'),\n",
       " np.str_('at'),\n",
       " np.str_('Quidditch,'),\n",
       " np.str_('but'),\n",
       " np.str_('I'),\n",
       " np.str_('bet'),\n",
       " np.str_(\"that's\"),\n",
       " np.str_('all'),\n",
       " np.str_('talk.\"'),\n",
       " '\\n\\n',\n",
       " np.str_('Malfoy'),\n",
       " np.str_('certainly'),\n",
       " np.str_('did'),\n",
       " np.str_('talk'),\n",
       " np.str_('about'),\n",
       " np.str_('flying'),\n",
       " np.str_('a'),\n",
       " np.str_('lot.'),\n",
       " np.str_('He'),\n",
       " np.str_('complained'),\n",
       " np.str_('loudly'),\n",
       " np.str_('about'),\n",
       " np.str_('first'),\n",
       " np.str_('years'),\n",
       " np.str_('never'),\n",
       " np.str_('getting'),\n",
       " np.str_('on'),\n",
       " np.str_('the'),\n",
       " np.str_('House'),\n",
       " np.str_('Quidditch'),\n",
       " np.str_('teams'),\n",
       " np.str_('and'),\n",
       " np.str_('told'),\n",
       " np.str_('long,'),\n",
       " np.str_('boastful'),\n",
       " np.str_('stories'),\n",
       " np.str_('that'),\n",
       " np.str_('always'),\n",
       " np.str_('seemed'),\n",
       " np.str_('to'),\n",
       " np.str_('end'),\n",
       " np.str_('with'),\n",
       " np.str_('him'),\n",
       " np.str_('narrowly'),\n",
       " np.str_('escaping'),\n",
       " np.str_('Muggles'),\n",
       " np.str_('in'),\n",
       " np.str_('helicopters.'),\n",
       " np.str_('He'),\n",
       " np.str_(\"wasn't\"),\n",
       " np.str_('the'),\n",
       " np.str_('only'),\n",
       " np.str_('one,'),\n",
       " np.str_('though:'),\n",
       " np.str_('the'),\n",
       " np.str_('way'),\n",
       " np.str_('Seamus'),\n",
       " np.str_('Finnigan'),\n",
       " np.str_('told'),\n",
       " np.str_('it,'),\n",
       " np.str_(\"he'd\"),\n",
       " np.str_('spent'),\n",
       " np.str_('most'),\n",
       " np.str_('of'),\n",
       " np.str_('his'),\n",
       " np.str_('childhood'),\n",
       " np.str_('zooming'),\n",
       " np.str_('around'),\n",
       " np.str_('the'),\n",
       " np.str_('countryside'),\n",
       " np.str_('on'),\n",
       " np.str_('his'),\n",
       " np.str_('broomstick.'),\n",
       " np.str_('Even'),\n",
       " np.str_('Ron'),\n",
       " np.str_('would'),\n",
       " np.str_('tell'),\n",
       " np.str_('anyone'),\n",
       " np.str_(\"who'd\"),\n",
       " np.str_('listen'),\n",
       " np.str_('about'),\n",
       " np.str_('the'),\n",
       " np.str_('time'),\n",
       " np.str_(\"he'd\"),\n",
       " np.str_('almost'),\n",
       " np.str_('hit'),\n",
       " np.str_('a'),\n",
       " np.str_('hang'),\n",
       " np.str_('glider'),\n",
       " np.str_('on'),\n",
       " np.str_(\"Charlie's\"),\n",
       " np.str_('old'),\n",
       " np.str_('broom.'),\n",
       " np.str_('Everyone'),\n",
       " np.str_('from'),\n",
       " np.str_('wizarding'),\n",
       " np.str_('families'),\n",
       " np.str_('talked'),\n",
       " np.str_('about'),\n",
       " np.str_('Quidditch'),\n",
       " np.str_('constantly.'),\n",
       " np.str_('Ron'),\n",
       " np.str_('had'),\n",
       " np.str_('already'),\n",
       " np.str_('had'),\n",
       " np.str_('a'),\n",
       " np.str_('big'),\n",
       " np.str_('argument'),\n",
       " np.str_('with'),\n",
       " np.str_('Dean'),\n",
       " np.str_('Thomas,'),\n",
       " np.str_('who'),\n",
       " np.str_('shared'),\n",
       " np.str_('their'),\n",
       " np.str_('dormitory,'),\n",
       " np.str_('about'),\n",
       " np.str_('soccer.'),\n",
       " np.str_('Ron'),\n",
       " np.str_(\"couldn't\"),\n",
       " np.str_('see'),\n",
       " np.str_('what'),\n",
       " np.str_('was'),\n",
       " np.str_('exciting'),\n",
       " np.str_('about'),\n",
       " np.str_('a'),\n",
       " np.str_('game'),\n",
       " np.str_('with'),\n",
       " np.str_('only'),\n",
       " np.str_('one'),\n",
       " np.str_('ball'),\n",
       " np.str_('where'),\n",
       " np.str_('no'),\n",
       " np.str_('one'),\n",
       " np.str_('was'),\n",
       " np.str_('allowed'),\n",
       " np.str_('to'),\n",
       " np.str_('fly.'),\n",
       " np.str_('Harry'),\n",
       " np.str_('had'),\n",
       " np.str_('caught'),\n",
       " np.str_('Ron'),\n",
       " np.str_('prodding'),\n",
       " np.str_(\"Dean's\"),\n",
       " np.str_('poster'),\n",
       " np.str_('of'),\n",
       " np.str_('West'),\n",
       " np.str_('Ham'),\n",
       " np.str_('soccer'),\n",
       " np.str_('team,'),\n",
       " np.str_('trying'),\n",
       " np.str_('to'),\n",
       " np.str_('make'),\n",
       " np.str_('the'),\n",
       " np.str_('players'),\n",
       " np.str_('move.'),\n",
       " '\\n\\n',\n",
       " np.str_('Neville'),\n",
       " np.str_('had'),\n",
       " np.str_('never'),\n",
       " np.str_('been'),\n",
       " np.str_('on'),\n",
       " np.str_('a'),\n",
       " np.str_('broomstick'),\n",
       " np.str_('in'),\n",
       " np.str_('his'),\n",
       " np.str_('life,'),\n",
       " np.str_('because'),\n",
       " np.str_('his'),\n",
       " np.str_('grandmother'),\n",
       " np.str_('had'),\n",
       " np.str_('never'),\n",
       " np.str_('let'),\n",
       " np.str_('him'),\n",
       " np.str_('near'),\n",
       " np.str_('one.'),\n",
       " np.str_('Privately,'),\n",
       " np.str_('Harry'),\n",
       " np.str_('felt'),\n",
       " np.str_(\"she'd\"),\n",
       " np.str_('had'),\n",
       " np.str_('good'),\n",
       " np.str_('reason,'),\n",
       " np.str_('because'),\n",
       " np.str_('Neville'),\n",
       " np.str_('managed'),\n",
       " np.str_('to'),\n",
       " np.str_('have'),\n",
       " np.str_('an'),\n",
       " np.str_('extraordinary'),\n",
       " np.str_('number'),\n",
       " np.str_('of'),\n",
       " np.str_('accidents'),\n",
       " np.str_('even'),\n",
       " np.str_('with'),\n",
       " np.str_('both'),\n",
       " np.str_('feet'),\n",
       " np.str_('on'),\n",
       " np.str_('the'),\n",
       " np.str_('ground.'),\n",
       " '\\n\\n',\n",
       " np.str_('Hermione'),\n",
       " np.str_('Granger'),\n",
       " np.str_('was'),\n",
       " np.str_('almost'),\n",
       " np.str_('as'),\n",
       " np.str_('nervous'),\n",
       " np.str_('about'),\n",
       " np.str_('flying'),\n",
       " np.str_('as'),\n",
       " np.str_('Neville'),\n",
       " np.str_('was.'),\n",
       " np.str_('This'),\n",
       " np.str_('was'),\n",
       " np.str_('something'),\n",
       " np.str_('you'),\n",
       " np.str_(\"couldn't\"),\n",
       " np.str_('learn'),\n",
       " np.str_('by'),\n",
       " np.str_('heart'),\n",
       " np.str_('out'),\n",
       " np.str_('of'),\n",
       " np.str_('a'),\n",
       " np.str_('book'),\n",
       " np.str_('--'),\n",
       " np.str_('not'),\n",
       " np.str_('that'),\n",
       " np.str_('she'),\n",
       " np.str_(\"hadn't\"),\n",
       " np.str_('tried.'),\n",
       " np.str_('At'),\n",
       " np.str_('breakfast'),\n",
       " np.str_('on'),\n",
       " np.str_('Thursday'),\n",
       " np.str_('she'),\n",
       " np.str_('bored'),\n",
       " np.str_('them'),\n",
       " np.str_('all'),\n",
       " np.str_('stupid'),\n",
       " np.str_('with'),\n",
       " np.str_('flying'),\n",
       " np.str_('tips'),\n",
       " np.str_(\"she'd\"),\n",
       " np.str_('gotten'),\n",
       " np.str_('out'),\n",
       " np.str_('of'),\n",
       " np.str_('a'),\n",
       " np.str_('library'),\n",
       " np.str_('book'),\n",
       " np.str_('called'),\n",
       " 'Quidditch',\n",
       " 'Through',\n",
       " 'the',\n",
       " 'Ages.',\n",
       " np.str_('Neville'),\n",
       " np.str_('was'),\n",
       " np.str_('hanging'),\n",
       " np.str_('on'),\n",
       " np.str_('to'),\n",
       " np.str_('her'),\n",
       " np.str_('every'),\n",
       " np.str_('word,'),\n",
       " np.str_('desperate'),\n",
       " np.str_('for'),\n",
       " np.str_('anything'),\n",
       " np.str_('that'),\n",
       " np.str_('might'),\n",
       " np.str_('help'),\n",
       " np.str_('him'),\n",
       " np.str_('hang'),\n",
       " np.str_('on'),\n",
       " np.str_('to'),\n",
       " np.str_('his'),\n",
       " np.str_('broomstick'),\n",
       " np.str_('later,'),\n",
       " np.str_('but'),\n",
       " np.str_('everybody'),\n",
       " np.str_('else'),\n",
       " np.str_('was'),\n",
       " np.str_('very'),\n",
       " np.str_('pleased'),\n",
       " np.str_('when'),\n",
       " np.str_(\"Hermione's\"),\n",
       " np.str_('lecture'),\n",
       " np.str_('was'),\n",
       " np.str_('interrupted'),\n",
       " np.str_('by'),\n",
       " np.str_('the'),\n",
       " np.str_('arrival'),\n",
       " np.str_('of'),\n",
       " np.str_('the'),\n",
       " np.str_('mail.'),\n",
       " '\\n\\n',\n",
       " np.str_('Harry'),\n",
       " np.str_(\"hadn't\"),\n",
       " np.str_('had'),\n",
       " np.str_('a'),\n",
       " np.str_('single'),\n",
       " np.str_('letter'),\n",
       " np.str_('since'),\n",
       " np.str_(\"Hagrid's\"),\n",
       " np.str_('note,'),\n",
       " np.str_('something'),\n",
       " np.str_('that'),\n",
       " np.str_('Malfoy'),\n",
       " np.str_('had'),\n",
       " np.str_('been'),\n",
       " np.str_('quick'),\n",
       " np.str_('to'),\n",
       " np.str_('notice,'),\n",
       " np.str_('of'),\n",
       " np.str_('course.'),\n",
       " np.str_(\"Malfoy's\"),\n",
       " np.str_('eagle'),\n",
       " np.str_('owl'),\n",
       " np.str_('was'),\n",
       " np.str_('always'),\n",
       " np.str_('bringing'),\n",
       " np.str_('him'),\n",
       " np.str_('packages'),\n",
       " np.str_('of'),\n",
       " np.str_('sweets'),\n",
       " np.str_('from'),\n",
       " np.str_('home,'),\n",
       " np.str_('which'),\n",
       " np.str_('he'),\n",
       " np.str_('opened'),\n",
       " np.str_('gloatingly'),\n",
       " np.str_('at'),\n",
       " np.str_('the'),\n",
       " np.str_('Slytherin'),\n",
       " np.str_('table.'),\n",
       " '\\n\\n',\n",
       " np.str_('A'),\n",
       " np.str_('barn'),\n",
       " np.str_('owl'),\n",
       " np.str_('brought'),\n",
       " np.str_('Neville'),\n",
       " np.str_('a'),\n",
       " np.str_('small'),\n",
       " np.str_('package'),\n",
       " np.str_('from'),\n",
       " np.str_('his'),\n",
       " np.str_('grandmother.'),\n",
       " np.str_('He'),\n",
       " np.str_('opened'),\n",
       " np.str_('it'),\n",
       " np.str_('excitedly'),\n",
       " np.str_('and'),\n",
       " np.str_('showed'),\n",
       " np.str_('them'),\n",
       " np.str_('a'),\n",
       " np.str_('glass'),\n",
       " np.str_('ball'),\n",
       " np.str_('the'),\n",
       " np.str_('size'),\n",
       " np.str_('of'),\n",
       " np.str_('a'),\n",
       " np.str_('large'),\n",
       " np.str_('marble,'),\n",
       " np.str_('which'),\n",
       " np.str_('seemed'),\n",
       " np.str_('to'),\n",
       " np.str_('be'),\n",
       " np.str_('full'),\n",
       " np.str_('of'),\n",
       " np.str_('white'),\n",
       " np.str_('smoke.'),\n",
       " '\\n\\n',\n",
       " np.str_('\"It\\'s'),\n",
       " np.str_('a'),\n",
       " np.str_('Remembrall!\"'),\n",
       " np.str_('he'),\n",
       " np.str_('explained.'),\n",
       " np.str_('\"Gran'),\n",
       " np.str_('knows'),\n",
       " np.str_('I'),\n",
       " np.str_('forget'),\n",
       " np.str_('things'),\n",
       " np.str_('--'),\n",
       " np.str_('this'),\n",
       " np.str_('tells'),\n",
       " np.str_('you'),\n",
       " np.str_('if'),\n",
       " np.str_(\"there's\"),\n",
       " np.str_('something'),\n",
       " np.str_(\"you've\"),\n",
       " np.str_('forgotten'),\n",
       " np.str_('to'),\n",
       " np.str_('do.'),\n",
       " np.str_('Look,'),\n",
       " np.str_('you'),\n",
       " np.str_('hold'),\n",
       " np.str_('it'),\n",
       " np.str_('tight'),\n",
       " np.str_('like'),\n",
       " np.str_('this'),\n",
       " np.str_('and'),\n",
       " np.str_('if'),\n",
       " np.str_('it'),\n",
       " np.str_('turns'),\n",
       " np.str_('red'),\n",
       " np.str_('--'),\n",
       " np.str_('oh'),\n",
       " np.str_('…\"'),\n",
       " np.str_('His'),\n",
       " np.str_('face'),\n",
       " np.str_('fell,'),\n",
       " np.str_('because'),\n",
       " np.str_('the'),\n",
       " np.str_('Remembrall'),\n",
       " np.str_('had'),\n",
       " np.str_('suddenly'),\n",
       " np.str_('glowed'),\n",
       " np.str_('scarlet,'),\n",
       " np.str_('\"…'),\n",
       " np.str_(\"you've\"),\n",
       " np.str_('forgotten'),\n",
       " np.str_('something'),\n",
       " np.str_('…\"'),\n",
       " '\\n\\n',\n",
       " np.str_('Neville'),\n",
       " np.str_('was'),\n",
       " np.str_('trying'),\n",
       " np.str_('to'),\n",
       " np.str_('remember'),\n",
       " np.str_('what'),\n",
       " np.str_(\"he'd\"),\n",
       " np.str_('forgotten'),\n",
       " np.str_('when'),\n",
       " np.str_('Draco'),\n",
       " np.str_('Malfoy,'),\n",
       " np.str_('who'),\n",
       " np.str_('was'),\n",
       " np.str_('passing'),\n",
       " np.str_('the'),\n",
       " np.str_('Gryffindor'),\n",
       " np.str_('table,'),\n",
       " np.str_('snatched'),\n",
       " np.str_('the'),\n",
       " np.str_('Remembrall'),\n",
       " np.str_('out'),\n",
       " np.str_('of'),\n",
       " np.str_('his'),\n",
       " np.str_('hand.'),\n",
       " '\\n\\n',\n",
       " np.str_('Harry'),\n",
       " np.str_('and'),\n",
       " np.str_('Ron'),\n",
       " np.str_('jumped'),\n",
       " np.str_('to'),\n",
       " np.str_('their'),\n",
       " np.str_('feet.'),\n",
       " np.str_('They'),\n",
       " np.str_('were'),\n",
       " np.str_('half'),\n",
       " np.str_('hoping'),\n",
       " np.str_('for'),\n",
       " np.str_('a'),\n",
       " np.str_('reason'),\n",
       " np.str_('to'),\n",
       " np.str_('fight'),\n",
       " np.str_('Malfoy,'),\n",
       " np.str_('but'),\n",
       " np.str_('Professor'),\n",
       " np.str_('McGonagall,'),\n",
       " np.str_('who'),\n",
       " np.str_('could'),\n",
       " np.str_('spot'),\n",
       " np.str_('trouble'),\n",
       " np.str_('quicker'),\n",
       " np.str_('than'),\n",
       " np.str_('any'),\n",
       " np.str_('teacher'),\n",
       " np.str_('in'),\n",
       " np.str_('the'),\n",
       " np.str_('school,'),\n",
       " np.str_('was'),\n",
       " np.str_('there'),\n",
       " np.str_('in'),\n",
       " np.str_('a'),\n",
       " np.str_('flash.'),\n",
       " '\\n\\n',\n",
       " np.str_('\"What\\'s'),\n",
       " np.str_('going'),\n",
       " np.str_('on?\"'),\n",
       " '\\n\\n',\n",
       " np.str_('\"Malfoy\\'s'),\n",
       " np.str_('got'),\n",
       " np.str_('my'),\n",
       " np.str_('Remembrall,'),\n",
       " np.str_('Professor.\"'),\n",
       " '\\n\\n',\n",
       " np.str_('Scowling,'),\n",
       " np.str_('Malfoy'),\n",
       " np.str_('quickly'),\n",
       " np.str_('dropped'),\n",
       " np.str_('the'),\n",
       " np.str_('Remembrall'),\n",
       " np.str_('back'),\n",
       " np.str_('on'),\n",
       " np.str_('the'),\n",
       " np.str_('table.'),\n",
       " '\\n\\n',\n",
       " np.str_('\"Just'),\n",
       " np.str_('looking,\"'),\n",
       " np.str_('he'),\n",
       " np.str_('said,'),\n",
       " np.str_('and'),\n",
       " np.str_('he'),\n",
       " np.str_('sloped'),\n",
       " np.str_('away'),\n",
       " np.str_('with'),\n",
       " np.str_('Crabbe'),\n",
       " np.str_('and'),\n",
       " np.str_('Goyle'),\n",
       " np.str_('behind'),\n",
       " np.str_('him.'),\n",
       " '\\n\\n',\n",
       " '\\n\\n',\n",
       " np.str_('At'),\n",
       " np.str_('three-thirty'),\n",
       " np.str_('that'),\n",
       " np.str_('afternoon,'),\n",
       " np.str_('Harry,'),\n",
       " np.str_('Ron,'),\n",
       " np.str_('and'),\n",
       " np.str_('the'),\n",
       " np.str_('other'),\n",
       " np.str_('Gryffindors'),\n",
       " np.str_('hurried'),\n",
       " np.str_('down'),\n",
       " np.str_('the'),\n",
       " np.str_('front'),\n",
       " np.str_('steps'),\n",
       " np.str_('onto'),\n",
       " np.str_('the'),\n",
       " np.str_('grounds'),\n",
       " np.str_('for'),\n",
       " np.str_('their'),\n",
       " np.str_('first'),\n",
       " np.str_('flying'),\n",
       " np.str_('lesson.'),\n",
       " np.str_('It'),\n",
       " np.str_('was'),\n",
       " np.str_('a'),\n",
       " np.str_('clear,'),\n",
       " np.str_('breezy'),\n",
       " np.str_('day,'),\n",
       " np.str_('and'),\n",
       " np.str_('the'),\n",
       " np.str_('grass'),\n",
       " np.str_('rippled'),\n",
       " np.str_('under'),\n",
       " np.str_('their'),\n",
       " np.str_('feet'),\n",
       " np.str_('as'),\n",
       " np.str_('they'),\n",
       " np.str_('marched'),\n",
       " np.str_('down'),\n",
       " np.str_('the'),\n",
       " np.str_('sloping'),\n",
       " np.str_('lawns'),\n",
       " np.str_('toward'),\n",
       " np.str_('a'),\n",
       " np.str_('smooth,'),\n",
       " np.str_('flat'),\n",
       " np.str_('lawn'),\n",
       " np.str_('on'),\n",
       " np.str_('the'),\n",
       " np.str_('opposite'),\n",
       " np.str_('side'),\n",
       " np.str_('of'),\n",
       " np.str_('the'),\n",
       " np.str_('grounds'),\n",
       " np.str_('to'),\n",
       " np.str_('the'),\n",
       " np.str_('forbidden'),\n",
       " np.str_('forest,'),\n",
       " np.str_('whose'),\n",
       " np.str_('trees'),\n",
       " np.str_('were'),\n",
       " np.str_('swaying'),\n",
       " np.str_('darkly'),\n",
       " np.str_('in'),\n",
       " np.str_('the'),\n",
       " np.str_('distance.'),\n",
       " '\\n\\n',\n",
       " np.str_('The'),\n",
       " np.str_('Slytherins'),\n",
       " np.str_('were'),\n",
       " np.str_('already'),\n",
       " np.str_('there,'),\n",
       " np.str_('and'),\n",
       " np.str_('so'),\n",
       " np.str_('were'),\n",
       " np.str_('twenty'),\n",
       " np.str_('broomsticks'),\n",
       " np.str_('lying'),\n",
       " np.str_('in'),\n",
       " np.str_('neat'),\n",
       " np.str_('lines'),\n",
       " np.str_('on'),\n",
       " np.str_('the'),\n",
       " np.str_('ground.'),\n",
       " np.str_('Harry'),\n",
       " np.str_('had'),\n",
       " np.str_('heard'),\n",
       " np.str_('Fred'),\n",
       " np.str_('and'),\n",
       " np.str_('George'),\n",
       " np.str_('Weasley'),\n",
       " np.str_('complain'),\n",
       " np.str_('about'),\n",
       " np.str_('the'),\n",
       " np.str_('school'),\n",
       " np.str_('brooms,'),\n",
       " np.str_('saying'),\n",
       " np.str_('that'),\n",
       " np.str_('some'),\n",
       " np.str_('of'),\n",
       " np.str_('them'),\n",
       " np.str_('started'),\n",
       " np.str_('to'),\n",
       " np.str_('vibrate'),\n",
       " np.str_('if'),\n",
       " np.str_('you'),\n",
       " np.str_('flew'),\n",
       " np.str_('too'),\n",
       " np.str_('high,'),\n",
       " np.str_('or'),\n",
       " np.str_('always'),\n",
       " np.str_('flew'),\n",
       " np.str_('slightly'),\n",
       " np.str_('to'),\n",
       " np.str_('the'),\n",
       " np.str_('left.'),\n",
       " '\\n\\n',\n",
       " np.str_('Their'),\n",
       " np.str_('teacher,'),\n",
       " np.str_('Madam'),\n",
       " np.str_('Hooch,'),\n",
       " np.str_('arrived.'),\n",
       " np.str_('She'),\n",
       " np.str_('had'),\n",
       " np.str_('short,'),\n",
       " np.str_('gray'),\n",
       " np.str_('hair,'),\n",
       " np.str_('and'),\n",
       " np.str_('yellow'),\n",
       " np.str_('eyes'),\n",
       " np.str_('like'),\n",
       " np.str_('a'),\n",
       " np.str_('hawk.'),\n",
       " '\\n\\n',\n",
       " np.str_('\"Well,'),\n",
       " np.str_('what'),\n",
       " np.str_('are'),\n",
       " np.str_('you'),\n",
       " np.str_('all'),\n",
       " np.str_('waiting'),\n",
       " np.str_('for?\"'),\n",
       " np.str_('she'),\n",
       " np.str_('barked.'),\n",
       " np.str_('\"Everyone'),\n",
       " np.str_('stand'),\n",
       " np.str_('by'),\n",
       " np.str_('a'),\n",
       " np.str_('broomstick.'),\n",
       " np.str_('Come'),\n",
       " np.str_('on,'),\n",
       " np.str_('hurry'),\n",
       " np.str_('up.\"'),\n",
       " '\\n\\n',\n",
       " np.str_('Harry'),\n",
       " np.str_('glanced'),\n",
       " np.str_('down'),\n",
       " np.str_('at'),\n",
       " np.str_('his'),\n",
       " np.str_('broom.'),\n",
       " np.str_('It'),\n",
       " np.str_('was'),\n",
       " np.str_('old'),\n",
       " np.str_('and'),\n",
       " np.str_('some'),\n",
       " np.str_('of'),\n",
       " np.str_('the'),\n",
       " np.str_('twigs'),\n",
       " np.str_('stuck'),\n",
       " np.str_('out'),\n",
       " np.str_('at'),\n",
       " np.str_('odd'),\n",
       " np.str_('angles.'),\n",
       " '\\n\\n',\n",
       " np.str_('\"Stick'),\n",
       " np.str_('out'),\n",
       " np.str_('your'),\n",
       " np.str_('right'),\n",
       " np.str_('hand'),\n",
       " np.str_('over'),\n",
       " np.str_('your'),\n",
       " np.str_('broom,\"'),\n",
       " np.str_('called'),\n",
       " np.str_('Madam'),\n",
       " np.str_('Hooch'),\n",
       " np.str_('at'),\n",
       " np.str_('the'),\n",
       " np.str_('front,'),\n",
       " np.str_('\"and'),\n",
       " np.str_('say'),\n",
       " np.str_('‘Up!\\'\"'),\n",
       " '\\n\\n',\n",
       " np.str_('\"UP!\"'),\n",
       " np.str_('everyone'),\n",
       " np.str_('shouted.'),\n",
       " '\\n\\n',\n",
       " np.str_(\"Harry's\"),\n",
       " np.str_('broom'),\n",
       " np.str_('jumped'),\n",
       " np.str_('into'),\n",
       " np.str_('his'),\n",
       " np.str_('hand'),\n",
       " np.str_('at'),\n",
       " np.str_('once,'),\n",
       " np.str_('but'),\n",
       " np.str_('it'),\n",
       " np.str_('was'),\n",
       " np.str_('one'),\n",
       " np.str_('of'),\n",
       " np.str_('the'),\n",
       " np.str_('few'),\n",
       " np.str_('that'),\n",
       " np.str_('did.'),\n",
       " np.str_('Hermione'),\n",
       " np.str_(\"Granger's\"),\n",
       " np.str_('had'),\n",
       " np.str_('simply'),\n",
       " np.str_('rolled'),\n",
       " np.str_('over'),\n",
       " np.str_('on'),\n",
       " np.str_('the'),\n",
       " np.str_('ground,'),\n",
       " np.str_('and'),\n",
       " np.str_(\"Neville's\"),\n",
       " np.str_(\"hadn't\"),\n",
       " np.str_('moved'),\n",
       " np.str_('at'),\n",
       " np.str_('all.'),\n",
       " np.str_('Perhaps'),\n",
       " np.str_('brooms,'),\n",
       " np.str_('like'),\n",
       " np.str_('horses,'),\n",
       " np.str_('could'),\n",
       " np.str_('tell'),\n",
       " np.str_('when'),\n",
       " np.str_('you'),\n",
       " np.str_('were'),\n",
       " np.str_('afraid,'),\n",
       " np.str_('thought'),\n",
       " np.str_('Harry;'),\n",
       " np.str_('there'),\n",
       " np.str_('was'),\n",
       " np.str_('a'),\n",
       " np.str_('quaver'),\n",
       " np.str_('in'),\n",
       " np.str_(\"Neville's\"),\n",
       " np.str_('voice'),\n",
       " np.str_('that'),\n",
       " np.str_('said'),\n",
       " np.str_('only'),\n",
       " np.str_('too'),\n",
       " np.str_('clearly'),\n",
       " np.str_('that'),\n",
       " np.str_('he'),\n",
       " np.str_('wanted'),\n",
       " np.str_('to'),\n",
       " np.str_('keep'),\n",
       " np.str_('his'),\n",
       " np.str_('feet'),\n",
       " np.str_('on'),\n",
       " np.str_('the'),\n",
       " np.str_('ground.'),\n",
       " '\\n\\n',\n",
       " np.str_('Madam'),\n",
       " np.str_('Hooch'),\n",
       " np.str_('then'),\n",
       " np.str_('showed'),\n",
       " np.str_('them'),\n",
       " np.str_('how'),\n",
       " np.str_('to'),\n",
       " np.str_('mount'),\n",
       " np.str_('their'),\n",
       " np.str_('brooms'),\n",
       " np.str_('without'),\n",
       " np.str_('sliding'),\n",
       " np.str_('off'),\n",
       " np.str_('the'),\n",
       " np.str_('end,'),\n",
       " np.str_('and'),\n",
       " np.str_('walked'),\n",
       " np.str_('up'),\n",
       " np.str_('and'),\n",
       " np.str_('down'),\n",
       " np.str_('the'),\n",
       " np.str_('rows'),\n",
       " np.str_('correcting'),\n",
       " np.str_('their'),\n",
       " np.str_('grips.'),\n",
       " np.str_('Harry'),\n",
       " np.str_('and'),\n",
       " np.str_('Ron'),\n",
       " np.str_('were'),\n",
       " np.str_('delighted'),\n",
       " np.str_('when'),\n",
       " np.str_('she'),\n",
       " np.str_('told'),\n",
       " np.str_('Malfoy'),\n",
       " np.str_(\"he'd\"),\n",
       " np.str_('been'),\n",
       " np.str_('doing'),\n",
       " np.str_('it'),\n",
       " np.str_('wrong'),\n",
       " np.str_('for'),\n",
       " np.str_('years.'),\n",
       " '\\n\\n',\n",
       " np.str_('\"Now,'),\n",
       " np.str_('when'),\n",
       " np.str_('I'),\n",
       " np.str_('blow'),\n",
       " np.str_('my'),\n",
       " np.str_('whistle,'),\n",
       " np.str_('you'),\n",
       " np.str_('kick'),\n",
       " np.str_('off'),\n",
       " np.str_('from'),\n",
       " np.str_('the'),\n",
       " np.str_('ground,'),\n",
       " np.str_('hard,\"'),\n",
       " np.str_('said'),\n",
       " np.str_('Madam'),\n",
       " np.str_('Hooch.'),\n",
       " np.str_('\"Keep'),\n",
       " np.str_('your'),\n",
       " np.str_('brooms'),\n",
       " np.str_('steady,'),\n",
       " np.str_('rise'),\n",
       " np.str_('a'),\n",
       " np.str_('few'),\n",
       " np.str_('feet,'),\n",
       " np.str_('and'),\n",
       " np.str_('then'),\n",
       " np.str_('come'),\n",
       " np.str_('straight'),\n",
       " np.str_('back'),\n",
       " np.str_('down'),\n",
       " np.str_('by'),\n",
       " np.str_('leaning'),\n",
       " np.str_('forward'),\n",
       " np.str_('slightly.'),\n",
       " np.str_('On'),\n",
       " np.str_('my'),\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLM_input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43e8851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_sequence, inverse = shuffle_words(LLM_input_sequence, percentage=1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fd9f2fd-445b-41fd-b530-824cb08002b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.str_('was'),\n",
       " np.str_('lying'),\n",
       " np.str_('back'),\n",
       " np.str_('Everyone'),\n",
       " np.str_('a'),\n",
       " np.str_('like'),\n",
       " np.str_('a'),\n",
       " np.str_('from'),\n",
       " np.str_('she'),\n",
       " np.str_('George'),\n",
       " np.str_('Told'),\n",
       " np.str_('you'),\n",
       " np.str_('like'),\n",
       " np.str_('safely'),\n",
       " np.str_('him'),\n",
       " np.str_('on'),\n",
       " np.str_(\"Flitwick's\"),\n",
       " np.str_('of'),\n",
       " np.str_('they'),\n",
       " np.str_('a'),\n",
       " np.str_('passageway,'),\n",
       " np.str_('lesson.'),\n",
       " np.str_('of'),\n",
       " np.str_('Crabbe'),\n",
       " np.str_('pig'),\n",
       " np.str_('lazily'),\n",
       " np.str_('to'),\n",
       " np.str_('all'),\n",
       " np.str_('this.\"'),\n",
       " np.str_('number'),\n",
       " np.str_('the'),\n",
       " np.str_('somewhere,'),\n",
       " np.str_('\"The'),\n",
       " np.str_(\"you'll\"),\n",
       " np.str_('\"This'),\n",
       " np.str_('a'),\n",
       " np.str_('barked.'),\n",
       " np.str_('\"Tonight,'),\n",
       " np.str_('up'),\n",
       " np.str_('streaked'),\n",
       " np.str_('gone'),\n",
       " np.str_('said.'),\n",
       " np.str_(\"you're\"),\n",
       " np.str_(\"We'll\"),\n",
       " np.str_('\"We\\'ve'),\n",
       " np.str_('see'),\n",
       " np.str_('tear-streaked,'),\n",
       " np.str_('Longbottom?\"'),\n",
       " np.str_('windows.'),\n",
       " np.str_('where'),\n",
       " np.str_('excitedly'),\n",
       " np.str_('he'),\n",
       " np.str_('gone,'),\n",
       " np.str_('his'),\n",
       " np.str_('\"Now,'),\n",
       " np.str_('looked'),\n",
       " np.str_('was'),\n",
       " np.str_('find'),\n",
       " np.str_('face-to-face.'),\n",
       " np.str_('meet'),\n",
       " np.str_('do'),\n",
       " np.str_('he'),\n",
       " np.str_('Wood'),\n",
       " np.str_('said'),\n",
       " np.str_('Gryffindors'),\n",
       " np.str_('They'),\n",
       " np.str_('from'),\n",
       " np.str_('Slytherin'),\n",
       " np.str_('fast'),\n",
       " np.str_('them'),\n",
       " np.str_('through'),\n",
       " np.str_('space'),\n",
       " np.str_('looking'),\n",
       " np.str_('him.'),\n",
       " np.str_('my'),\n",
       " np.str_('was'),\n",
       " np.str_('you'),\n",
       " np.str_('see'),\n",
       " np.str_('your'),\n",
       " '\\n\\n',\n",
       " np.str_('and'),\n",
       " np.str_(\"didn't\"),\n",
       " np.str_('under'),\n",
       " np.str_('the'),\n",
       " np.str_('Neville'),\n",
       " np.str_('by'),\n",
       " np.str_('Neville'),\n",
       " np.str_('away'),\n",
       " np.str_('too'),\n",
       " np.str_('way'),\n",
       " np.str_('told'),\n",
       " np.str_('Harry'),\n",
       " np.str_('broom'),\n",
       " np.str_('had'),\n",
       " np.str_('you!\"'),\n",
       " np.str_('almost'),\n",
       " np.str_('size'),\n",
       " np.str_('heard'),\n",
       " np.str_('as'),\n",
       " np.str_('boy'),\n",
       " np.str_('you,'),\n",
       " np.str_('eyes'),\n",
       " '\\n\\n',\n",
       " np.str_('up'),\n",
       " np.str_('he'),\n",
       " np.str_('grabbed'),\n",
       " np.str_('they'),\n",
       " np.str_('and'),\n",
       " np.str_('What'),\n",
       " np.str_('he'),\n",
       " np.str_('for'),\n",
       " np.str_('\"Keep'),\n",
       " np.str_('Professor'),\n",
       " np.str_('\"Anyway,'),\n",
       " np.str_('with'),\n",
       " np.str_('knew'),\n",
       " np.str_(\"Goyle's\"),\n",
       " np.str_('with'),\n",
       " np.str_('know,'),\n",
       " np.str_('and'),\n",
       " np.str_('bathrobes,'),\n",
       " np.str_('Wood'),\n",
       " np.str_('just'),\n",
       " np.str_('they'),\n",
       " np.str_('whipped'),\n",
       " np.str_('said'),\n",
       " np.str_('right,'),\n",
       " np.str_('what'),\n",
       " np.str_('McGonagall'),\n",
       " np.str_('and'),\n",
       " np.str_('\"Your'),\n",
       " np.str_('for'),\n",
       " np.str_('the'),\n",
       " np.str_('Filch'),\n",
       " np.str_('die,\"'),\n",
       " np.str_('snout,'),\n",
       " np.str_('\"The'),\n",
       " np.str_('of'),\n",
       " np.str_('for'),\n",
       " np.str_('the'),\n",
       " np.str_('caught'),\n",
       " np.str_(\"don't\"),\n",
       " np.str_('please'),\n",
       " np.str_('eyes'),\n",
       " np.str_('see'),\n",
       " np.str_('Bloody'),\n",
       " np.str_('your'),\n",
       " np.str_('pointed'),\n",
       " np.str_('watching'),\n",
       " '\\n\\n',\n",
       " np.str_('Thomas,'),\n",
       " np.str_('the'),\n",
       " np.str_('kept'),\n",
       " np.str_('shouted,'),\n",
       " np.str_('\"probably'),\n",
       " np.str_('silver'),\n",
       " np.str_('Jordan'),\n",
       " np.str_('knuckles'),\n",
       " np.str_('seemed'),\n",
       " np.str_('\"Absolutely,\"'),\n",
       " np.str_('But'),\n",
       " np.str_(\"second's\"),\n",
       " np.str_('them'),\n",
       " np.str_('and'),\n",
       " np.str_('a'),\n",
       " np.str_('they'),\n",
       " np.str_('of'),\n",
       " np.str_('to'),\n",
       " np.str_('as'),\n",
       " np.str_('I'),\n",
       " np.str_('other'),\n",
       " np.str_('my'),\n",
       " np.str_('got'),\n",
       " np.str_('facing'),\n",
       " np.str_('for'),\n",
       " np.str_('be'),\n",
       " np.str_('going'),\n",
       " np.str_('dive,\"'),\n",
       " np.str_('dived.'),\n",
       " np.str_('Ron'),\n",
       " np.str_('he'),\n",
       " np.str_('at'),\n",
       " np.str_('he'),\n",
       " np.str_('out.\"'),\n",
       " np.str_('But'),\n",
       " np.str_('been'),\n",
       " np.str_('higher'),\n",
       " np.str_('and'),\n",
       " np.str_('interrupted'),\n",
       " np.str_(\"year's\"),\n",
       " np.str_('speak'),\n",
       " np.str_('do'),\n",
       " np.str_('a'),\n",
       " np.str_('what'),\n",
       " np.str_('rule.'),\n",
       " np.str_('book'),\n",
       " np.str_('Pansy'),\n",
       " np.str_('clanged'),\n",
       " np.str_('toward'),\n",
       " np.str_('got'),\n",
       " np.str_('WHAM'),\n",
       " np.str_('looking'),\n",
       " np.str_('Harry'),\n",
       " np.str_('perfect'),\n",
       " np.str_('thunderous'),\n",
       " np.str_('must'),\n",
       " np.str_('around.'),\n",
       " np.str_('he'),\n",
       " np.str_('themselves'),\n",
       " np.str_('little'),\n",
       " np.str_('cases'),\n",
       " np.str_('think'),\n",
       " np.str_('the'),\n",
       " np.str_('They'),\n",
       " np.str_('a'),\n",
       " np.str_('fight'),\n",
       " np.str_('filled'),\n",
       " np.str_('ground.'),\n",
       " np.str_('that'),\n",
       " np.str_('scrambled'),\n",
       " np.str_('burly'),\n",
       " np.str_('darkness'),\n",
       " np.str_('for'),\n",
       " np.str_('open.'),\n",
       " np.str_('it,'),\n",
       " np.str_('nothing'),\n",
       " np.str_('nearer.'),\n",
       " np.str_('wake'),\n",
       " np.str_('inside,'),\n",
       " np.str_('--'),\n",
       " np.str_('lock,'),\n",
       " np.str_('of'),\n",
       " np.str_('off'),\n",
       " np.str_('door'),\n",
       " np.str_('tell'),\n",
       " np.str_('If'),\n",
       " np.str_('frightened'),\n",
       " np.str_('the'),\n",
       " np.str_('got'),\n",
       " np.str_('\"In'),\n",
       " np.str_('my'),\n",
       " np.str_('It'),\n",
       " np.str_('which'),\n",
       " np.str_('crisply.'),\n",
       " np.str_('white'),\n",
       " np.str_('on'),\n",
       " np.str_('\"Which'),\n",
       " np.str_('he'),\n",
       " np.str_('ears.'),\n",
       " np.str_('George'),\n",
       " np.str_('for'),\n",
       " np.str_('taking'),\n",
       " np.str_('Crabbe'),\n",
       " np.str_('between'),\n",
       " np.str_('think'),\n",
       " np.str_('when'),\n",
       " np.str_('--'),\n",
       " np.str_('speaking'),\n",
       " np.str_('his'),\n",
       " np.str_('the'),\n",
       " np.str_('a'),\n",
       " np.str_('on'),\n",
       " np.str_('then'),\n",
       " np.str_('Harry'),\n",
       " np.str_('spot'),\n",
       " np.str_('to'),\n",
       " np.str_('his'),\n",
       " np.str_('he'),\n",
       " np.str_('with'),\n",
       " np.str_('say'),\n",
       " np.str_('it.'),\n",
       " np.str_('and'),\n",
       " np.str_('help'),\n",
       " np.str_('a'),\n",
       " np.str_('and'),\n",
       " np.str_('you'),\n",
       " np.str_('expelled,'),\n",
       " np.str_('on'),\n",
       " np.str_('out'),\n",
       " np.str_('get'),\n",
       " np.str_('you'),\n",
       " np.str_('to'),\n",
       " np.str_('on'),\n",
       " np.str_('if'),\n",
       " np.str_('their'),\n",
       " np.str_('course'),\n",
       " np.str_('jumped'),\n",
       " np.str_('course.'),\n",
       " np.str_('Neville,'),\n",
       " np.str_('on'),\n",
       " np.str_('face,'),\n",
       " np.str_('slightly'),\n",
       " np.str_('\"That\\'s'),\n",
       " np.str_('punishing'),\n",
       " np.str_('Ham'),\n",
       " '\\n\\n',\n",
       " np.str_('getting'),\n",
       " np.str_('voice.'),\n",
       " np.str_('there'),\n",
       " np.str_('raised'),\n",
       " np.str_('forward'),\n",
       " np.str_('Harry'),\n",
       " np.str_('to'),\n",
       " np.str_('their'),\n",
       " np.str_('it'),\n",
       " np.str_('and'),\n",
       " np.str_('Granger,'),\n",
       " np.str_('room.'),\n",
       " np.str_('but'),\n",
       " np.str_('at'),\n",
       " np.str_('up'),\n",
       " np.str_('trotting'),\n",
       " np.str_('and'),\n",
       " np.str_('reckons'),\n",
       " np.str_('robes'),\n",
       " np.str_('her'),\n",
       " np.str_('there'),\n",
       " np.str_('excitedly.'),\n",
       " np.str_('She'),\n",
       " np.str_('locked.'),\n",
       " np.str_('hand'),\n",
       " np.str_('reason'),\n",
       " np.str_('away.\"'),\n",
       " np.str_('quite'),\n",
       " np.str_('sure'),\n",
       " np.str_('Potter,'),\n",
       " np.str_('of'),\n",
       " np.str_('right'),\n",
       " np.str_('running'),\n",
       " np.str_('and'),\n",
       " np.str_('keep'),\n",
       " np.str_('out'),\n",
       " np.str_('a'),\n",
       " np.str_(\"you'll\"),\n",
       " np.str_('steps'),\n",
       " np.str_('almost'),\n",
       " np.str_('how'),\n",
       " np.str_('without'),\n",
       " np.str_('the'),\n",
       " '\\n\\n',\n",
       " np.str_('flickered'),\n",
       " np.str_('he'),\n",
       " np.str_('flashed'),\n",
       " np.str_('where'),\n",
       " np.str_('Gryffindor'),\n",
       " np.str_('between'),\n",
       " np.str_('business,\"'),\n",
       " np.str_('Ron'),\n",
       " np.str_('as'),\n",
       " np.str_('wanted'),\n",
       " np.str_('to'),\n",
       " np.str_('Lady'),\n",
       " np.str_('things'),\n",
       " np.str_(\"Dean's\"),\n",
       " np.str_('himself,'),\n",
       " np.str_('hope'),\n",
       " np.str_('The'),\n",
       " np.str_('end,'),\n",
       " np.str_('her'),\n",
       " np.str_('he'),\n",
       " np.str_('your'),\n",
       " np.str_('left.'),\n",
       " np.str_('forbidden.'),\n",
       " np.str_('was'),\n",
       " np.str_('creep'),\n",
       " np.str_('Hermione'),\n",
       " np.str_('Ha'),\n",
       " np.str_('on'),\n",
       " np.str_('Parvati'),\n",
       " np.str_('bellowed,'),\n",
       " np.str_('that'),\n",
       " np.str_('the'),\n",
       " np.str_('Neville!\"'),\n",
       " np.str_('trouble'),\n",
       " np.str_('lessons'),\n",
       " np.str_('crept'),\n",
       " np.str_('\"And'),\n",
       " np.str_('broom.'),\n",
       " np.str_('was'),\n",
       " np.str_(\"you're\"),\n",
       " np.str_('face'),\n",
       " np.str_('Fat'),\n",
       " np.str_('Harry'),\n",
       " np.str_('--'),\n",
       " np.str_('you'),\n",
       " np.str_('wand,'),\n",
       " np.str_('pie.'),\n",
       " np.str_('as'),\n",
       " np.str_('sloping'),\n",
       " np.str_('Malfoy'),\n",
       " np.str_('use'),\n",
       " np.str_('shared'),\n",
       " np.str_('the'),\n",
       " np.str_('to'),\n",
       " np.str_('\"Catch'),\n",
       " np.str_('his'),\n",
       " np.str_('jump.'),\n",
       " np.str_('happens?\"'),\n",
       " np.str_('Ron'),\n",
       " np.str_('eyes;'),\n",
       " np.str_('do'),\n",
       " np.str_('caught,'),\n",
       " np.str_('toward'),\n",
       " np.str_('muttered'),\n",
       " np.str_('McGonagall'),\n",
       " np.str_('forgotten'),\n",
       " np.str_('have'),\n",
       " np.str_('a'),\n",
       " np.str_('Ron'),\n",
       " np.str_('it'),\n",
       " np.str_('all'),\n",
       " np.str_('held'),\n",
       " np.str_(\"weren't\"),\n",
       " np.str_('in'),\n",
       " np.str_('scrambling'),\n",
       " np.str_('if'),\n",
       " np.str_('Still,'),\n",
       " np.str_('boys.'),\n",
       " np.str_('the'),\n",
       " np.str_('glasses'),\n",
       " np.str_('becoming'),\n",
       " np.str_('right,'),\n",
       " np.str_('was'),\n",
       " np.str_('had'),\n",
       " np.str_('sharp'),\n",
       " np.str_('caughty.\"'),\n",
       " np.str_('the'),\n",
       " np.str_('It'),\n",
       " np.str_('the'),\n",
       " np.str_('eyes'),\n",
       " np.str_('you'),\n",
       " '\\n\\n',\n",
       " np.str_('Crabbe'),\n",
       " np.str_('a'),\n",
       " np.str_('doing'),\n",
       " np.str_('century,\"'),\n",
       " np.str_('up.\"'),\n",
       " np.str_('us'),\n",
       " np.str_(\"we've\"),\n",
       " np.str_(\"that's\"),\n",
       " np.str_('she'),\n",
       " np.str_('three'),\n",
       " np.str_('saliva'),\n",
       " np.str_('packing'),\n",
       " np.str_('\"I'),\n",
       " np.str_('neck'),\n",
       " np.str_('was'),\n",
       " np.str_('ran,'),\n",
       " np.str_('Ron'),\n",
       " np.str_('had'),\n",
       " np.str_('to'),\n",
       " np.str_('afternoon.'),\n",
       " np.str_('Flying'),\n",
       " np.str_('They'),\n",
       " np.str_('the'),\n",
       " np.str_('trophy'),\n",
       " np.str_('He'),\n",
       " np.str_('Professor'),\n",
       " np.str_('that'),\n",
       " np.str_('down'),\n",
       " np.str_('Harry'),\n",
       " np.str_('almost'),\n",
       " np.str_('better'),\n",
       " np.str_('life,'),\n",
       " np.str_('been'),\n",
       " np.str_('the'),\n",
       " np.str_('now'),\n",
       " np.str_('off'),\n",
       " np.str_('do?\"'),\n",
       " np.str_('Harry.'),\n",
       " np.str_('left'),\n",
       " '\\n\\n',\n",
       " np.str_('you'),\n",
       " np.str_('in'),\n",
       " np.str_('of'),\n",
       " np.str_('to'),\n",
       " np.str_('pinned'),\n",
       " np.str_('what'),\n",
       " np.str_('constantly.'),\n",
       " np.str_('Wood'),\n",
       " np.str_('a'),\n",
       " np.str_('knew'),\n",
       " np.str_('Ron'),\n",
       " np.str_('look'),\n",
       " np.str_('a'),\n",
       " np.str_('to'),\n",
       " np.str_('--'),\n",
       " np.str_('go,'),\n",
       " np.str_('have'),\n",
       " np.str_('at'),\n",
       " np.str_('open'),\n",
       " np.str_('round'),\n",
       " np.str_('you,\"'),\n",
       " np.str_('years'),\n",
       " np.str_('--'),\n",
       " np.str_('gone'),\n",
       " np.str_('them.'),\n",
       " np.str_('room,'),\n",
       " np.str_('rude'),\n",
       " np.str_('and'),\n",
       " np.str_('an'),\n",
       " np.str_('year.'),\n",
       " np.str_('said'),\n",
       " np.str_('silently.'),\n",
       " np.str_('annoying'),\n",
       " np.str_('picked'),\n",
       " np.str_('the'),\n",
       " '\\n\\n',\n",
       " np.str_('to'),\n",
       " np.str_('tightly'),\n",
       " np.str_('you'),\n",
       " np.str_('face'),\n",
       " np.str_('Malfoy'),\n",
       " np.str_('and'),\n",
       " np.str_('what'),\n",
       " np.str_('was'),\n",
       " np.str_('into'),\n",
       " np.str_('them.'),\n",
       " np.str_('Hooch'),\n",
       " np.str_('dodge'),\n",
       " np.str_('on,'),\n",
       " np.str_('doors'),\n",
       " np.str_('friends'),\n",
       " np.str_('they'),\n",
       " np.str_('wants'),\n",
       " np.str_('talk.\"'),\n",
       " np.str_('Hermione'),\n",
       " np.str_('this'),\n",
       " np.str_('he'),\n",
       " np.str_('the'),\n",
       " np.str_('have'),\n",
       " np.str_('OUT'),\n",
       " np.str_('you.\"'),\n",
       " np.str_('Seven,'),\n",
       " np.str_('on'),\n",
       " np.str_('enough'),\n",
       " np.str_('in'),\n",
       " np.str_('glittered'),\n",
       " np.str_('down'),\n",
       " np.str_('had'),\n",
       " np.str_('Parvati.\"'),\n",
       " np.str_('be'),\n",
       " np.str_('well.'),\n",
       " np.str_('writing'),\n",
       " np.str_('\"Crabbe,\"'),\n",
       " np.str_('into'),\n",
       " np.str_('had'),\n",
       " np.str_('to'),\n",
       " np.str_('--'),\n",
       " np.str_('it'),\n",
       " np.str_('up.\"'),\n",
       " np.str_('and'),\n",
       " np.str_('Ha'),\n",
       " np.str_('back'),\n",
       " np.str_('game'),\n",
       " np.str_('and'),\n",
       " np.str_(\"don't\"),\n",
       " np.str_('past'),\n",
       " '\"Never',\n",
       " '\\n\\n',\n",
       " np.str_('swung'),\n",
       " np.str_('followed'),\n",
       " np.str_('bathrobe'),\n",
       " np.str_('Harry'),\n",
       " np.str_('whispered,'),\n",
       " np.str_('hanging'),\n",
       " np.str_('to'),\n",
       " np.str_('the'),\n",
       " np.str_('Their'),\n",
       " np.str_('toward'),\n",
       " np.str_('his'),\n",
       " np.str_('No'),\n",
       " np.str_('lot'),\n",
       " np.str_('the'),\n",
       " np.str_('was'),\n",
       " np.str_('looked'),\n",
       " np.str_('had'),\n",
       " np.str_(\"can't\"),\n",
       " np.str_('They'),\n",
       " np.str_('from'),\n",
       " np.str_('exercise,'),\n",
       " np.str_('the'),\n",
       " np.str_('up,'),\n",
       " np.str_('Wood'),\n",
       " '\\n\\n',\n",
       " np.str_('\"All'),\n",
       " np.str_('much,'),\n",
       " np.str_('if'),\n",
       " np.str_('\"Shan\\'t'),\n",
       " np.str_('in'),\n",
       " np.str_('around'),\n",
       " np.str_('screams'),\n",
       " np.str_('edged'),\n",
       " np.str_('end'),\n",
       " np.str_('She'),\n",
       " np.str_('\"Bet'),\n",
       " np.str_('out.'),\n",
       " np.str_('Peeves,'),\n",
       " np.str_('distance.'),\n",
       " np.str_('who'),\n",
       " np.str_('a'),\n",
       " np.str_('stay'),\n",
       " np.str_('Tower,\"'),\n",
       " np.str_('glittered'),\n",
       " np.str_('get'),\n",
       " np.str_('shut,'),\n",
       " np.str_('his'),\n",
       " np.str_('to'),\n",
       " np.str_('goose.'),\n",
       " np.str_('the'),\n",
       " np.str_('a'),\n",
       " np.str_(\"Malfoy's\"),\n",
       " np.str_('in'),\n",
       " np.str_('guarding'),\n",
       " np.str_('was'),\n",
       " np.str_('from'),\n",
       " np.str_('at'),\n",
       " np.str_('around'),\n",
       " np.str_('build'),\n",
       " np.str_('a'),\n",
       " np.str_('heads.\"'),\n",
       " '\\n\\n',\n",
       " np.str_('that'),\n",
       " np.str_('she'),\n",
       " np.str_('eagle'),\n",
       " '\\n\\n',\n",
       " np.str_('had'),\n",
       " np.str_('is'),\n",
       " np.str_('do'),\n",
       " np.str_('it,'),\n",
       " np.str_('grasped'),\n",
       " '\\n\\n',\n",
       " np.str_('Come'),\n",
       " np.str_('first-year'),\n",
       " np.str_('was'),\n",
       " np.str_('McGonagall'),\n",
       " np.str_('mouth,'),\n",
       " np.str_('seemed'),\n",
       " np.str_('their'),\n",
       " np.str_('still'),\n",
       " np.str_('one'),\n",
       " np.str_('a'),\n",
       " '\\n\\n',\n",
       " np.str_('Fat'),\n",
       " np.str_('and'),\n",
       " np.str_('always'),\n",
       " np.str_('the'),\n",
       " np.str_('the'),\n",
       " np.str_('side'),\n",
       " np.str_('to'),\n",
       " np.str_('the'),\n",
       " np.str_('when'),\n",
       " np.str_('way'),\n",
       " np.str_('the'),\n",
       " np.str_('to'),\n",
       " np.str_('I'),\n",
       " np.str_('nearer.'),\n",
       " '\\n\\n',\n",
       " '\\n\\n',\n",
       " np.str_('to'),\n",
       " np.str_('To'),\n",
       " np.str_('a'),\n",
       " np.str_('the'),\n",
       " np.str_('\"Midnight'),\n",
       " np.str_('and'),\n",
       " np.str_('him'),\n",
       " np.str_('had'),\n",
       " np.str_('stand'),\n",
       " np.str_('any'),\n",
       " np.str_('\"Ooh,'),\n",
       " np.str_('said'),\n",
       " np.str_('shot'),\n",
       " np.str_('he'),\n",
       " np.str_('to'),\n",
       " np.str_('Harry'),\n",
       " np.str_('bed.\"'),\n",
       " np.str_('is'),\n",
       " np.str_('and'),\n",
       " np.str_('Cleansweep'),\n",
       " np.str_('move.'),\n",
       " np.str_(\"wizard's\"),\n",
       " np.str_('brooms'),\n",
       " np.str_('down'),\n",
       " np.str_('anything'),\n",
       " np.str_('\"Can\\'t'),\n",
       " np.str_('turned'),\n",
       " np.str_('poster'),\n",
       " np.str_('a'),\n",
       " np.str_('whole'),\n",
       " np.str_('the'),\n",
       " np.str_('mount'),\n",
       " np.str_('soccer'),\n",
       " np.str_('of'),\n",
       " np.str_('because'),\n",
       " np.str_('\"and'),\n",
       " np.str_('the'),\n",
       " np.str_('what'),\n",
       " np.str_('cursing'),\n",
       " np.str_('of'),\n",
       " np.str_('a'),\n",
       " '\"You!\"',\n",
       " np.str_('\"Shut'),\n",
       " np.str_('the'),\n",
       " np.str_('down'),\n",
       " np.str_('sparks'),\n",
       " np.str_('onto'),\n",
       " np.str_('\"Light'),\n",
       " np.str_('on'),\n",
       " np.str_('password'),\n",
       " np.str_('killed'),\n",
       " np.str_('what'),\n",
       " np.str_('wrong'),\n",
       " np.str_('high,'),\n",
       " np.str_('slow'),\n",
       " np.str_('was'),\n",
       " np.str_('Harry'),\n",
       " np.str_('looked'),\n",
       " np.str_('you'),\n",
       " np.str_('have'),\n",
       " np.str_(\"don't\"),\n",
       " np.str_('reached'),\n",
       " np.str_('they'),\n",
       " np.str_('Harry'),\n",
       " np.str_('win'),\n",
       " np.str_('at'),\n",
       " np.str_('knew'),\n",
       " '\\n\\n',\n",
       " np.str_('sped'),\n",
       " np.str_('to'),\n",
       " np.str_('of'),\n",
       " np.str_('hurtled'),\n",
       " np.str_('save'),\n",
       " np.str_('scratch'),\n",
       " np.str_(\"hadn't\"),\n",
       " np.str_('at'),\n",
       " np.str_('dive,'),\n",
       " np.str_('everyone'),\n",
       " np.str_('what'),\n",
       " np.str_('just'),\n",
       " np.str_('standing'),\n",
       " np.str_('staircase,'),\n",
       " np.str_('there,'),\n",
       " np.str_('sure'),\n",
       " np.str_('visit'),\n",
       " np.str_('most'),\n",
       " np.str_('forbidden'),\n",
       " np.str_('going'),\n",
       " np.str_('tell'),\n",
       " np.str_('come'),\n",
       " np.str_('a'),\n",
       " np.str_('was'),\n",
       " np.str_('yelled,'),\n",
       " np.str_('ears'),\n",
       " np.str_('once,'),\n",
       " np.str_('moved'),\n",
       " np.str_('his'),\n",
       " np.str_('At'),\n",
       " np.str_(\"who'd\"),\n",
       " np.str_('and'),\n",
       " np.str_('stretched'),\n",
       " np.str_('--'),\n",
       " np.str_('The'),\n",
       " np.str_('Remembrall'),\n",
       " np.str_('toppled'),\n",
       " np.str_('learning'),\n",
       " np.str_('nighttime'),\n",
       " np.str_('feet'),\n",
       " np.str_('Hermione'),\n",
       " np.str_(\"you're\"),\n",
       " np.str_('how'),\n",
       " np.str_('steep'),\n",
       " np.str_('broomstick'),\n",
       " np.str_('next'),\n",
       " np.str_('darkly.'),\n",
       " np.str_('hear'),\n",
       " np.str_('they'),\n",
       " '\\n\\n',\n",
       " np.str_('any'),\n",
       " np.str_('when'),\n",
       " np.str_('it'),\n",
       " np.str_('late,'),\n",
       " np.str_('of'),\n",
       " np.str_('Harry.'),\n",
       " np.str_('galloped'),\n",
       " np.str_('they'),\n",
       " np.str_('her.'),\n",
       " np.str_('rattled'),\n",
       " np.str_('‘Up!\\'\"'),\n",
       " np.str_(\"couldn't\"),\n",
       " np.str_('yours?\"'),\n",
       " np.str_('Hermione'),\n",
       " np.str_('down'),\n",
       " np.str_('excellent'),\n",
       " '\\n\\n',\n",
       " np.str_(\"wouldn't\"),\n",
       " np.str_('You'),\n",
       " np.str_(\"that's\"),\n",
       " np.str_('eyes,'),\n",
       " np.str_('\"I'),\n",
       " np.str_('BED!\"'),\n",
       " np.str_('that'),\n",
       " np.str_('The'),\n",
       " np.str_('McGonagall'),\n",
       " np.str_('escaping'),\n",
       " np.str_('I'),\n",
       " np.str_('a'),\n",
       " np.str_('Harry.\"'),\n",
       " np.str_('monster.'),\n",
       " np.str_('that'),\n",
       " np.str_('stitch'),\n",
       " np.str_('wall'),\n",
       " np.str_('\"Mrs.'),\n",
       " '\\n\\n',\n",
       " np.str_('--'),\n",
       " np.str_('say.\"'),\n",
       " np.str_('already'),\n",
       " np.str_('along'),\n",
       " np.str_('to'),\n",
       " np.str_('us'),\n",
       " np.str_('on'),\n",
       " np.str_('got'),\n",
       " np.str_('Harry'),\n",
       " np.str_('mutter,'),\n",
       " np.str_('getting'),\n",
       " np.str_(\"he'd\"),\n",
       " np.str_('enough'),\n",
       " np.str_('in'),\n",
       " np.str_('speechless'),\n",
       " np.str_('our'),\n",
       " np.str_('run'),\n",
       " np.str_('he'),\n",
       " np.str_('before'),\n",
       " '\\n\\n',\n",
       " np.str_('Then'),\n",
       " np.str_('trapdoor.'),\n",
       " np.str_(\"wizard's\"),\n",
       " np.str_('a'),\n",
       " np.str_('be'),\n",
       " np.str_('to'),\n",
       " np.str_('Remembrall'),\n",
       " np.str_('to'),\n",
       " np.str_('was'),\n",
       " np.str_('because'),\n",
       " np.str_('the'),\n",
       " np.str_('a'),\n",
       " np.str_('some'),\n",
       " np.str_('them'),\n",
       " np.str_('shields,'),\n",
       " np.str_('Mr.'),\n",
       " np.str_('full'),\n",
       " '\\n\\n',\n",
       " np.str_('could,\"'),\n",
       " np.str_('room'),\n",
       " np.str_('caught'),\n",
       " '\\n\\n',\n",
       " np.str_('The'),\n",
       " np.str_('here'),\n",
       " np.str_('--'),\n",
       " np.str_('Seamus'),\n",
       " np.str_('to'),\n",
       " np.str_('\"Did'),\n",
       " np.str_('the'),\n",
       " np.str_('soccer.'),\n",
       " np.str_(\"you're\"),\n",
       " np.str_('boy'),\n",
       " np.str_('teacher,'),\n",
       " np.str_('behind'),\n",
       " np.str_('--'),\n",
       " np.str_('A'),\n",
       " np.str_('staring'),\n",
       " np.str_('own'),\n",
       " np.str_('the'),\n",
       " np.str_('be'),\n",
       " np.str_('wandering'),\n",
       " np.str_('four'),\n",
       " np.str_('Oliver'),\n",
       " np.str_('broomstick.'),\n",
       " np.str_('ground.'),\n",
       " np.str_('\"This'),\n",
       " np.str_('feet.'),\n",
       " np.str_('But'),\n",
       " np.str_('off.\"'),\n",
       " np.str_('Neville'),\n",
       " np.str_('the'),\n",
       " np.str_('me.\"'),\n",
       " np.str_('what'),\n",
       " np.str_('scared'),\n",
       " np.str_('his'),\n",
       " np.str_('with'),\n",
       " np.str_('to'),\n",
       " np.str_('but'),\n",
       " np.str_('He'),\n",
       " np.str_('up'),\n",
       " np.str_('Patil'),\n",
       " np.str_('who'),\n",
       " np.str_('on'),\n",
       " np.str_('this'),\n",
       " np.str_('broom'),\n",
       " np.str_('of'),\n",
       " np.str_('it.\"'),\n",
       " np.str_(\"didn't\"),\n",
       " np.str_('Ron'),\n",
       " np.str_('but'),\n",
       " np.str_('given'),\n",
       " np.str_('how'),\n",
       " np.str_('his'),\n",
       " np.str_('room.'),\n",
       " np.str_('Peeves'),\n",
       " 'go?\"',\n",
       " np.str_('to'),\n",
       " np.str_('Madam'),\n",
       " np.str_(\"wasn't\"),\n",
       " np.str_('other'),\n",
       " np.str_('to'),\n",
       " np.str_('you'),\n",
       " np.str_('\"I'),\n",
       " np.str_('door'),\n",
       " np.str_('by'),\n",
       " np.str_('told'),\n",
       " np.str_('tut,'),\n",
       " np.str_('to'),\n",
       " '\\n\\n',\n",
       " np.str_('was'),\n",
       " np.str_('everybody'),\n",
       " np.str_('\"You\\'d'),\n",
       " np.str_('almost'),\n",
       " np.str_('air'),\n",
       " np.str_('numbly'),\n",
       " np.str_('Harry'),\n",
       " np.str_('hair,'),\n",
       " np.str_('Madam'),\n",
       " np.str_('Peeves!\"'),\n",
       " np.str_('the'),\n",
       " np.str_('It'),\n",
       " np.str_('a'),\n",
       " np.str_('warned'),\n",
       " np.str_('carrying'),\n",
       " np.str_('Goyle,'),\n",
       " np.str_('like'),\n",
       " np.str_('the'),\n",
       " np.str_('said'),\n",
       " np.str_('going'),\n",
       " np.str_('was'),\n",
       " np.str_('told'),\n",
       " np.str_('on'),\n",
       " np.str_('be'),\n",
       " np.str_('you'),\n",
       " np.str_('last'),\n",
       " np.str_('my'),\n",
       " np.str_('him'),\n",
       " np.str_('school.\"'),\n",
       " np.str_('broom,'),\n",
       " np.str_('hard'),\n",
       " np.str_('it,'),\n",
       " np.str_('them'),\n",
       " np.str_('Madam'),\n",
       " np.str_('Malfoy,'),\n",
       " np.str_('Crabbe'),\n",
       " np.str_('back'),\n",
       " np.str_('in'),\n",
       " np.str_('Remembrall!\"'),\n",
       " np.str_('die'),\n",
       " np.str_('He'),\n",
       " '\\n\\n',\n",
       " np.str_('but'),\n",
       " np.str_('of'),\n",
       " np.str_('she'),\n",
       " np.str_('over,\"'),\n",
       " np.str_('Harry'),\n",
       " np.str_('out'),\n",
       " np.str_('\"All'),\n",
       " np.str_('touched'),\n",
       " np.str_('and'),\n",
       " np.str_('turns'),\n",
       " np.str_('shock,'),\n",
       " np.str_('had'),\n",
       " np.str_('Wood'),\n",
       " np.str_('hang'),\n",
       " np.str_('I'),\n",
       " np.str_('suit'),\n",
       " np.str_('than'),\n",
       " np.str_('just'),\n",
       " np.str_('by'),\n",
       " np.str_(\"wasn't\"),\n",
       " np.str_('The'),\n",
       " np.str_('broomstick.'),\n",
       " np.str_('but'),\n",
       " np.str_('last'),\n",
       " np.str_('follow'),\n",
       " np.str_('Ron,'),\n",
       " np.str_('about'),\n",
       " np.str_('enough,'),\n",
       " np.str_('door,'),\n",
       " np.str_('looking'),\n",
       " np.str_('were'),\n",
       " np.str_('them'),\n",
       " np.str_('as'),\n",
       " np.str_('\"Excuse'),\n",
       " np.str_('each'),\n",
       " np.str_('door'),\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "2d9c4242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5176"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "a494a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "seed_value=42\n",
    "# Track the positions of special tokens to exclude from shuffling\n",
    "shuffle_indices = [i for i, w in enumerate(LLM_input_sequence) if w not in [\"\\n\\n\"]]\n",
    "# Extract and shuffle the words to be shuffled\n",
    "words_to_shuffle = [LLM_input_sequence[i] for i in shuffle_indices]\n",
    "shuffled_words = words_to_shuffle.copy()\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "random.shuffle(shuffled_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "73d64c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4975"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shuffle_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "27fda54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild the full sequence with shuffled words and preserved special tokens\n",
    "final_sequence = LLM_input_sequence.copy()\n",
    "for idx, word_idx in enumerate(shuffle_indices):\n",
    "    final_sequence[word_idx] = shuffled_words[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "36057a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5176"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(LLM_input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "fea6c52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5176"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "2a819fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_inverse_indices(original: List[str], shuffled: List[str]) -> List[int]:\n",
    "    from collections import defaultdict\n",
    "    used = defaultdict(int)\n",
    "    index_map = defaultdict(list)\n",
    "    \n",
    "    # Map all occurrences of each word to their positions in the original\n",
    "    for i, word in enumerate(original):\n",
    "        index_map[word].append(i)\n",
    "\n",
    "    indices = []\n",
    "    for word in shuffled:\n",
    "        idx = index_map[word][used[word]]\n",
    "        indices.append(idx)\n",
    "        used[word] += 1\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "2dae0352",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_indices = compute_inverse_indices(original=final_sequence, shuffled=LLM_input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "501df1e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[252,\n",
       " 1538,\n",
       " 1507,\n",
       " 3,\n",
       " 5021,\n",
       " 23,\n",
       " 3142,\n",
       " 218,\n",
       " 394,\n",
       " 3431,\n",
       " 934,\n",
       " 3815,\n",
       " 39,\n",
       " 25,\n",
       " 666,\n",
       " 47,\n",
       " 1495,\n",
       " 1192,\n",
       " 44,\n",
       " 129,\n",
       " 894,\n",
       " 357,\n",
       " 119,\n",
       " 1344,\n",
       " 1412,\n",
       " 3614,\n",
       " 130,\n",
       " 349,\n",
       " 62,\n",
       " 478,\n",
       " 82,\n",
       " 503,\n",
       " 246,\n",
       " 1894,\n",
       " 131,\n",
       " 768,\n",
       " 549,\n",
       " 71,\n",
       " 242,\n",
       " 64,\n",
       " 31,\n",
       " 32,\n",
       " 2,\n",
       " 158,\n",
       " 76,\n",
       " 147,\n",
       " 495,\n",
       " 33,\n",
       " 60,\n",
       " 146,\n",
       " 70,\n",
       " 645,\n",
       " 435,\n",
       " 2054,\n",
       " 1434,\n",
       " 281,\n",
       " 45,\n",
       " 104,\n",
       " 4976,\n",
       " 153,\n",
       " 3801,\n",
       " 517,\n",
       " 1237,\n",
       " 1699,\n",
       " 1743,\n",
       " 575,\n",
       " 900,\n",
       " 1150,\n",
       " 2692,\n",
       " 150,\n",
       " 237,\n",
       " 113,\n",
       " 1294,\n",
       " 497,\n",
       " 21,\n",
       " 1621,\n",
       " 54,\n",
       " 34,\n",
       " 368,\n",
       " 79,\n",
       " 1827,\n",
       " 771,\n",
       " 874,\n",
       " 1173,\n",
       " 756,\n",
       " 363,\n",
       " 109,\n",
       " 477,\n",
       " 208,\n",
       " 228,\n",
       " 84,\n",
       " 1910,\n",
       " 72,\n",
       " 618,\n",
       " 125,\n",
       " 5046,\n",
       " 194,\n",
       " 154,\n",
       " 447,\n",
       " 852,\n",
       " 542,\n",
       " 4424,\n",
       " 102,\n",
       " 343,\n",
       " 2883,\n",
       " 245,\n",
       " 583,\n",
       " 839,\n",
       " 36,\n",
       " 576,\n",
       " 718,\n",
       " 3208,\n",
       " 607,\n",
       " 108,\n",
       " 231,\n",
       " 1318,\n",
       " 780,\n",
       " 4433,\n",
       " 1720,\n",
       " 210,\n",
       " 4121,\n",
       " 327,\n",
       " 1689,\n",
       " 2132,\n",
       " 192,\n",
       " 1392,\n",
       " 457,\n",
       " 2295,\n",
       " 183,\n",
       " 3762,\n",
       " 562,\n",
       " 138,\n",
       " 264,\n",
       " 2373,\n",
       " 2996,\n",
       " 1041,\n",
       " 1929,\n",
       " 1362,\n",
       " 3997,\n",
       " 2366,\n",
       " 1637,\n",
       " 1005,\n",
       " 55,\n",
       " 1451,\n",
       " 170,\n",
       " 85,\n",
       " 52,\n",
       " 136,\n",
       " 211,\n",
       " 149,\n",
       " 1321,\n",
       " 2304,\n",
       " 268,\n",
       " 761,\n",
       " 10,\n",
       " 399,\n",
       " 2551,\n",
       " 92,\n",
       " 49,\n",
       " 3166,\n",
       " 480,\n",
       " 4562,\n",
       " 176,\n",
       " 2074,\n",
       " 4001,\n",
       " 663,\n",
       " 87,\n",
       " 2185,\n",
       " 74,\n",
       " 552,\n",
       " 620,\n",
       " 184,\n",
       " 112,\n",
       " 917,\n",
       " 396,\n",
       " 446,\n",
       " 508,\n",
       " 2170,\n",
       " 1870,\n",
       " 1156,\n",
       " 77,\n",
       " 2047,\n",
       " 325,\n",
       " 739,\n",
       " 699,\n",
       " 1483,\n",
       " 93,\n",
       " 1054,\n",
       " 1876,\n",
       " 3549,\n",
       " 2260,\n",
       " 4120,\n",
       " 421,\n",
       " 2165,\n",
       " 1528,\n",
       " 2336,\n",
       " 9,\n",
       " 1087,\n",
       " 271,\n",
       " 243,\n",
       " 755,\n",
       " 2229,\n",
       " 2768,\n",
       " 770,\n",
       " 409,\n",
       " 161,\n",
       " 590,\n",
       " 933,\n",
       " 416,\n",
       " 555,\n",
       " 120,\n",
       " 1795,\n",
       " 190,\n",
       " 1515,\n",
       " 800,\n",
       " 41,\n",
       " 103,\n",
       " 18,\n",
       " 3585,\n",
       " 1899,\n",
       " 115,\n",
       " 2200,\n",
       " 449,\n",
       " 97,\n",
       " 118,\n",
       " 128,\n",
       " 749,\n",
       " 1051,\n",
       " 259,\n",
       " 48,\n",
       " 2442,\n",
       " 462,\n",
       " 135,\n",
       " 275,\n",
       " 623,\n",
       " 785,\n",
       " 98,\n",
       " 165,\n",
       " 126,\n",
       " 37,\n",
       " 162,\n",
       " 152,\n",
       " 279,\n",
       " 1642,\n",
       " 1506,\n",
       " 1625,\n",
       " 950,\n",
       " 5162,\n",
       " 1539,\n",
       " 148,\n",
       " 163,\n",
       " 3854,\n",
       " 3761,\n",
       " 1132,\n",
       " 187,\n",
       " 2460,\n",
       " 1691,\n",
       " 899,\n",
       " 2650,\n",
       " 80,\n",
       " 1255,\n",
       " 342,\n",
       " 1610,\n",
       " 1048,\n",
       " 2422,\n",
       " 593,\n",
       " 107,\n",
       " 744,\n",
       " 2945,\n",
       " 527,\n",
       " 960,\n",
       " 197,\n",
       " 296,\n",
       " 1317,\n",
       " 591,\n",
       " 189,\n",
       " 204,\n",
       " 3249,\n",
       " 1826,\n",
       " 88,\n",
       " 3130,\n",
       " 2224,\n",
       " 1230,\n",
       " 175,\n",
       " 3482,\n",
       " 2419,\n",
       " 573,\n",
       " 509,\n",
       " 214,\n",
       " 1452,\n",
       " 670,\n",
       " 856,\n",
       " 976,\n",
       " 1866,\n",
       " 1598,\n",
       " 295,\n",
       " 344,\n",
       " 65,\n",
       " 258,\n",
       " 1138,\n",
       " 354,\n",
       " 202,\n",
       " 139,\n",
       " 2426,\n",
       " 116,\n",
       " 2896,\n",
       " 1038,\n",
       " 3412,\n",
       " 2230,\n",
       " 578,\n",
       " 4172,\n",
       " 3019,\n",
       " 3960,\n",
       " 599,\n",
       " 1460,\n",
       " 1705,\n",
       " 3470,\n",
       " 3228,\n",
       " 3212,\n",
       " 2153,\n",
       " 923,\n",
       " 4767,\n",
       " 414,\n",
       " 251,\n",
       " 172,\n",
       " 277,\n",
       " 427,\n",
       " 114,\n",
       " 1755,\n",
       " 4875,\n",
       " 998,\n",
       " 380,\n",
       " 180,\n",
       " 94,\n",
       " 600,\n",
       " 355,\n",
       " 2853,\n",
       " 341,\n",
       " 822,\n",
       " 339,\n",
       " 928,\n",
       " 4341,\n",
       " 203,\n",
       " 3053,\n",
       " 1067,\n",
       " 308,\n",
       " 303,\n",
       " 554,\n",
       " 430,\n",
       " 2032,\n",
       " 169,\n",
       " 470,\n",
       " 3586,\n",
       " 648,\n",
       " 630,\n",
       " 270,\n",
       " 181,\n",
       " 504,\n",
       " 1750,\n",
       " 1732,\n",
       " 5131,\n",
       " 466,\n",
       " 311,\n",
       " 802,\n",
       " 374,\n",
       " 2799,\n",
       " 4815,\n",
       " 289,\n",
       " 451,\n",
       " 1002,\n",
       " 595,\n",
       " 2564,\n",
       " 235,\n",
       " 1007,\n",
       " 876,\n",
       " 4134,\n",
       " 1415,\n",
       " 4430,\n",
       " 22,\n",
       " 1219,\n",
       " 5119,\n",
       " 263,\n",
       " 1016,\n",
       " 4677,\n",
       " 487,\n",
       " 205,\n",
       " 929,\n",
       " 982,\n",
       " 2096,\n",
       " 494,\n",
       " 514,\n",
       " 1427,\n",
       " 565,\n",
       " 419,\n",
       " 1976,\n",
       " 1488,\n",
       " 42,\n",
       " 710,\n",
       " 459,\n",
       " 300,\n",
       " 222,\n",
       " 304,\n",
       " 582,\n",
       " 219,\n",
       " 373,\n",
       " 2140,\n",
       " 167,\n",
       " 849,\n",
       " 479,\n",
       " 1812,\n",
       " 639,\n",
       " 732,\n",
       " 1509,\n",
       " 2876,\n",
       " 1977,\n",
       " 463,\n",
       " 393,\n",
       " 701,\n",
       " 423,\n",
       " 250,\n",
       " 241,\n",
       " 4819,\n",
       " 404,\n",
       " 312,\n",
       " 864,\n",
       " 441,\n",
       " 896,\n",
       " 4293,\n",
       " 185,\n",
       " 1685,\n",
       " 1182,\n",
       " 431,\n",
       " 4561,\n",
       " 96,\n",
       " 1072,\n",
       " 1966,\n",
       " 530,\n",
       " 156,\n",
       " 1265,\n",
       " 307,\n",
       " 2808,\n",
       " 468,\n",
       " 4056,\n",
       " 2147,\n",
       " 2202,\n",
       " 556,\n",
       " 1320,\n",
       " 1394,\n",
       " 598,\n",
       " 4946,\n",
       " 4298,\n",
       " 981,\n",
       " 403,\n",
       " 2591,\n",
       " 558,\n",
       " 367,\n",
       " 299,\n",
       " 687,\n",
       " 1155,\n",
       " 678,\n",
       " 455,\n",
       " 3167,\n",
       " 315,\n",
       " 232,\n",
       " 199,\n",
       " 2111,\n",
       " 1692,\n",
       " 2533,\n",
       " 635,\n",
       " 1500,\n",
       " 519,\n",
       " 471,\n",
       " 249,\n",
       " 722,\n",
       " 2378,\n",
       " 2403,\n",
       " 248,\n",
       " 326,\n",
       " 376,\n",
       " 213,\n",
       " 122,\n",
       " 548,\n",
       " 880,\n",
       " 753,\n",
       " 550,\n",
       " 1198,\n",
       " 2603,\n",
       " 651,\n",
       " 523,\n",
       " 4974,\n",
       " 520,\n",
       " 640,\n",
       " 2399,\n",
       " 655,\n",
       " 266,\n",
       " 1214,\n",
       " 1787,\n",
       " 1422,\n",
       " 1551,\n",
       " 99,\n",
       " 4392,\n",
       " 878,\n",
       " 566,\n",
       " 569,\n",
       " 2933,\n",
       " 3791,\n",
       " 579,\n",
       " 507,\n",
       " 3850,\n",
       " 272,\n",
       " 2958,\n",
       " 622,\n",
       " 693,\n",
       " 1353,\n",
       " 365,\n",
       " 757,\n",
       " 638,\n",
       " 1341,\n",
       " 4613,\n",
       " 1607,\n",
       " 2331,\n",
       " 1382,\n",
       " 262,\n",
       " 3179,\n",
       " 411,\n",
       " 4195,\n",
       " 660,\n",
       " 2498,\n",
       " 597,\n",
       " 2392,\n",
       " 1311,\n",
       " 3209,\n",
       " 345,\n",
       " 2899,\n",
       " 4619,\n",
       " 525,\n",
       " 1074,\n",
       " 1768,\n",
       " 253,\n",
       " 127,\n",
       " 3811,\n",
       " 586,\n",
       " 611,\n",
       " 3162,\n",
       " 200,\n",
       " 324,\n",
       " 953,\n",
       " 2259,\n",
       " 323,\n",
       " 201,\n",
       " 4176,\n",
       " 649,\n",
       " 347,\n",
       " 892,\n",
       " 437,\n",
       " 2480,\n",
       " 2757,\n",
       " 2539,\n",
       " 356,\n",
       " 559,\n",
       " 182,\n",
       " 1183,\n",
       " 1089,\n",
       " 2410,\n",
       " 1909,\n",
       " 2462,\n",
       " 464,\n",
       " 159,\n",
       " 912,\n",
       " 3611,\n",
       " 316,\n",
       " 4558,\n",
       " 4847,\n",
       " 4523,\n",
       " 650,\n",
       " 3337,\n",
       " 1612,\n",
       " 450,\n",
       " 679,\n",
       " 1802,\n",
       " 691,\n",
       " 2267,\n",
       " 4703,\n",
       " 38,\n",
       " 584,\n",
       " 134,\n",
       " 381,\n",
       " 3488,\n",
       " 444,\n",
       " 3332,\n",
       " 402,\n",
       " 456,\n",
       " 95,\n",
       " 2961,\n",
       " 5103,\n",
       " 848,\n",
       " 938,\n",
       " 965,\n",
       " 1865,\n",
       " 29,\n",
       " 783,\n",
       " 681,\n",
       " 2241,\n",
       " 1012,\n",
       " 1859,\n",
       " 66,\n",
       " 290,\n",
       " 16,\n",
       " 240,\n",
       " 484,\n",
       " 692,\n",
       " 198,\n",
       " 721,\n",
       " 469,\n",
       " 3818,\n",
       " 2063,\n",
       " 335,\n",
       " 1711,\n",
       " 297,\n",
       " 3113,\n",
       " 2711,\n",
       " 621,\n",
       " 1849,\n",
       " 3174,\n",
       " 2453,\n",
       " 625,\n",
       " 4382,\n",
       " 3993,\n",
       " 927,\n",
       " 4928,\n",
       " 433,\n",
       " 631,\n",
       " 674,\n",
       " 706,\n",
       " 872,\n",
       " 3775,\n",
       " 4199,\n",
       " 2252,\n",
       " 1165,\n",
       " 3448,\n",
       " 2478,\n",
       " 2994,\n",
       " 642,\n",
       " 2126,\n",
       " 267,\n",
       " 3273,\n",
       " 2271,\n",
       " 1004,\n",
       " 4634,\n",
       " 3467,\n",
       " 736,\n",
       " 1104,\n",
       " 426,\n",
       " 492,\n",
       " 837,\n",
       " 1778,\n",
       " 747,\n",
       " 657,\n",
       " 658,\n",
       " 962,\n",
       " 716,\n",
       " 1047,\n",
       " 879,\n",
       " 1042,\n",
       " 2144,\n",
       " 276,\n",
       " 3181,\n",
       " 1201,\n",
       " 2785,\n",
       " 5116,\n",
       " 2790,\n",
       " 3012,\n",
       " 4884,\n",
       " 816,\n",
       " 3438,\n",
       " 1603,\n",
       " 501,\n",
       " 518,\n",
       " 2565,\n",
       " 777,\n",
       " 392,\n",
       " 2992,\n",
       " 905,\n",
       " 1987,\n",
       " 510,\n",
       " 528,\n",
       " 4564,\n",
       " 4874,\n",
       " 604,\n",
       " 3032,\n",
       " 2886,\n",
       " 881,\n",
       " 857,\n",
       " 561,\n",
       " 891,\n",
       " 443,\n",
       " 536,\n",
       " 4125,\n",
       " 3973,\n",
       " 612,\n",
       " 790,\n",
       " 2016,\n",
       " 1330,\n",
       " 4582,\n",
       " 1177,\n",
       " 2239,\n",
       " 371,\n",
       " 835,\n",
       " 609,\n",
       " 2237,\n",
       " 4688,\n",
       " 123,\n",
       " 1122,\n",
       " 4404,\n",
       " 2672,\n",
       " 617,\n",
       " 337,\n",
       " 774,\n",
       " 821,\n",
       " 2998,\n",
       " 1093,\n",
       " 581,\n",
       " 3035,\n",
       " 2954,\n",
       " 567,\n",
       " 1978,\n",
       " 726,\n",
       " 361,\n",
       " 168,\n",
       " 5166,\n",
       " 3793,\n",
       " 1170,\n",
       " 3486,\n",
       " 574,\n",
       " 1176,\n",
       " 1404,\n",
       " 1880,\n",
       " 227,\n",
       " 834,\n",
       " 725,\n",
       " 553,\n",
       " 2086,\n",
       " 3443,\n",
       " 915,\n",
       " 1020,\n",
       " 1167,\n",
       " 723,\n",
       " 475,\n",
       " 2397,\n",
       " 1079,\n",
       " 1567,\n",
       " 2203,\n",
       " 221,\n",
       " 1833,\n",
       " 286,\n",
       " 1092,\n",
       " 369,\n",
       " 2526,\n",
       " 652,\n",
       " 417,\n",
       " 3613,\n",
       " 537,\n",
       " 643,\n",
       " 3094,\n",
       " 636,\n",
       " 57,\n",
       " 1268,\n",
       " 734,\n",
       " 1482,\n",
       " 667,\n",
       " 742,\n",
       " 1256,\n",
       " 1580,\n",
       " 705,\n",
       " 1141,\n",
       " 223,\n",
       " 776,\n",
       " 956,\n",
       " 454,\n",
       " 992,\n",
       " 1962,\n",
       " 750,\n",
       " 3013,\n",
       " 4684,\n",
       " 224,\n",
       " 1096,\n",
       " 1591,\n",
       " 1236,\n",
       " 2296,\n",
       " 425,\n",
       " 811,\n",
       " 294,\n",
       " 424,\n",
       " 793,\n",
       " 428,\n",
       " 760,\n",
       " 737,\n",
       " 4018,\n",
       " 788,\n",
       " 1028,\n",
       " 1324,\n",
       " 1073,\n",
       " 1476,\n",
       " 1484,\n",
       " 1740,\n",
       " 1152,\n",
       " 694,\n",
       " 2078,\n",
       " 389,\n",
       " 410,\n",
       " 2523,\n",
       " 5135,\n",
       " 812,\n",
       " 448,\n",
       " 4952,\n",
       " 1728,\n",
       " 698,\n",
       " 724,\n",
       " 513,\n",
       " 614,\n",
       " 1366,\n",
       " 633,\n",
       " 580,\n",
       " 2251,\n",
       " 616,\n",
       " 988,\n",
       " 4945,\n",
       " 2858,\n",
       " 1517,\n",
       " 544,\n",
       " 2026,\n",
       " 1550,\n",
       " 832,\n",
       " 4391,\n",
       " 283,\n",
       " 1564,\n",
       " 941,\n",
       " 1211,\n",
       " 3233,\n",
       " 1204,\n",
       " 773,\n",
       " 1936,\n",
       " 1274,\n",
       " 711,\n",
       " 1521,\n",
       " 63,\n",
       " 233,\n",
       " 702,\n",
       " 5114,\n",
       " 515,\n",
       " 850,\n",
       " 1823,\n",
       " 1885,\n",
       " 1069,\n",
       " 854,\n",
       " 1304,\n",
       " 58,\n",
       " 360,\n",
       " 2821,\n",
       " 390,\n",
       " 1537,\n",
       " 1025,\n",
       " 3093,\n",
       " 1944,\n",
       " 2447,\n",
       " 594,\n",
       " 1281,\n",
       " 2369,\n",
       " 86,\n",
       " 522,\n",
       " 2391,\n",
       " 1566,\n",
       " 2647,\n",
       " 828,\n",
       " 1871,\n",
       " 752,\n",
       " 1445,\n",
       " 637,\n",
       " 1158,\n",
       " 491,\n",
       " 949,\n",
       " 1292,\n",
       " 422,\n",
       " 420,\n",
       " 870,\n",
       " 4214,\n",
       " 256,\n",
       " 274,\n",
       " 570,\n",
       " 2030,\n",
       " 2289,\n",
       " 3844,\n",
       " 1136,\n",
       " 209,\n",
       " 3450,\n",
       " 1776,\n",
       " 4124,\n",
       " 1030,\n",
       " 526,\n",
       " 4831,\n",
       " 808,\n",
       " 967,\n",
       " 866,\n",
       " 3716,\n",
       " 765,\n",
       " 914,\n",
       " 1043,\n",
       " 825,\n",
       " 322,\n",
       " 516,\n",
       " 1127,\n",
       " 2435,\n",
       " 75,\n",
       " 439,\n",
       " 919,\n",
       " 2427,\n",
       " 603,\n",
       " 686,\n",
       " 1355,\n",
       " 709,\n",
       " 1406,\n",
       " 921,\n",
       " 1970,\n",
       " 1652,\n",
       " 1133,\n",
       " 2018,\n",
       " 577,\n",
       " 809,\n",
       " 2482,\n",
       " 2344,\n",
       " 1497,\n",
       " 1229,\n",
       " 1263,\n",
       " 1314,\n",
       " 1431,\n",
       " 1793,\n",
       " 332,\n",
       " 5041,\n",
       " 2290,\n",
       " 841,\n",
       " 5016,\n",
       " 4185,\n",
       " 3653,\n",
       " 810,\n",
       " 1811,\n",
       " 2625,\n",
       " 196,\n",
       " 3227,\n",
       " 3957,\n",
       " 2188,\n",
       " 3641,\n",
       " 220,\n",
       " 2039,\n",
       " 4277,\n",
       " 707,\n",
       " 2010,\n",
       " 1393,\n",
       " 733,\n",
       " 2775,\n",
       " 661,\n",
       " 813,\n",
       " 4548,\n",
       " 1190,\n",
       " 963,\n",
       " 1045,\n",
       " 1199,\n",
       " 972,\n",
       " 4024,\n",
       " 453,\n",
       " 1026,\n",
       " 2449,\n",
       " 3004,\n",
       " 713,\n",
       " 1673,\n",
       " 4303,\n",
       " 2110,\n",
       " 3403,\n",
       " 1272,\n",
       " 925,\n",
       " 284,\n",
       " 226,\n",
       " 4219,\n",
       " 1733,\n",
       " 2261,\n",
       " 767,\n",
       " 766,\n",
       " 646,\n",
       " 1009,\n",
       " 3270,\n",
       " 2505,\n",
       " 3202,\n",
       " 1581,\n",
       " 2587,\n",
       " 641,\n",
       " 2930,\n",
       " 845,\n",
       " 4055,\n",
       " 244,\n",
       " 3165,\n",
       " 1270,\n",
       " ...]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "97538d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse_indices_old = [None] * len(words_to_shuffle)\n",
    "# used = [False] * len(words_to_shuffle)\n",
    "\n",
    "# for i, word in enumerate(shuffled_words):\n",
    "#     for j, (orig_word, flag) in enumerate(zip(words_to_shuffle, used)):\n",
    "#         if not flag and word == orig_word:\n",
    "#             inverse_indices[i] = shuffle_indices[j]\n",
    "#             used[j] = True\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "84b50753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(inverse_indices_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "17fa47ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Choose one of the following (set only one to True)\n",
    "# shuffle_paragraphs_flag = True\n",
    "# shuffle_sentences_flag = False\n",
    "# shuffle_words_flag = True\n",
    "\n",
    "# if shuffle_words_flag:\n",
    "#     LLM_input_sequence, inverse_indices = shuffle_words(LLM_input_sequence)\n",
    "# else:\n",
    "#     inverse_indices = list(range(len(LLM_input_sequence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f648bb",
   "metadata": {},
   "source": [
    "## 2. compute LLM representations for these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "ee051339",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_input_sequence=shuffled_sequence.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "bbd6b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to a writable location, like /tmp or your home dir\n",
    "HF_home = \"/tmp/huggingface_cache\"  # or \"/home/youruser/.cache/huggingface\"\n",
    "\n",
    "os.makedirs(HF_home, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "0894014d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model and tokenizer.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, cache_dir=HF_home)\n",
    "model = AutoModel.from_pretrained(model_name, cache_dir=HF_home).to(\"cpu\")\n",
    "print(\"Loaded model and tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "4c974265",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings_to_find = LLM_input_sequence # we search for these words in sequential order in the full prompt to obtain avg representations for these.\n",
    "prompt_text = \" \".join(LLM_input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "124e23ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Harry had never school few would meet a boy of her. whispered, than constantly. but that was flying he met Draco and Still, first-year Gryffindors only now Potions with more Slytherins, so they which have to put up thing Malfoy much. Or at least, they Malfoy, until they if a notice pinned up gray the Gryffindor common Patil felt made there all \"RUN!\" Flying toward would be starting on Thursday -- and Gryffindor and Slytherin would broom. learning together. \\n\\n obviously said Harry darkly. any what I always Wood To Quidditch a fool of myself shouted, a broomstick in front of could \\n\\n He knows, \"Half-past looking wasn\\'t to POTTER!\" to fly more than \"But else. \"You just know that in make grandmother. fool of yourself,\" said Ron reasonably. McGonagall, I know Malfoy\\'s always going on wiping how and he is at Quidditch, pushed I bet that\\'s all talk.\" \\n\\n feet. certainly did talk lump?\" fist. a lot. He complained loudly course we snatched in getting on the House from teams and hear long, boastf',\n",
       " 28670)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_text[:1000], len(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "b6be7b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed word level representations for the input sequence.\n"
     ]
    }
   ],
   "source": [
    "# compute representations \n",
    "occurrences, representations = get_ordered_representations(\n",
    "    ordered_strings_to_find=strings_to_find,\n",
    "    prompt=prompt_text,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model\n",
    ")\n",
    "print(\"Computed word level representations for the input sequence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "4c9f93f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5176, 2048])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representations[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408eafa1",
   "metadata": {},
   "source": [
    "## 3. Map from word-level LLM representations to TR level representations via Lanczos resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "2c155988",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = -7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "4310b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: Reorder the model output (shuffled order) back to original order using inverse_indices\n",
    "reordered_representations = representations[layer_idx][inverse_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "97a25f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# word_times must be ordered the same as reordered_representations\n",
    "reordered_word_times = word_times[inverse_indices]  # Make sure this works after fixing inverse_indices\n",
    "\n",
    "# Ensure representation and word_times match\n",
    "assert reordered_representations.shape[0] == reordered_word_times.shape[0], \"Mismatch in # of tokens and timings\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "53a0f7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0677, -0.0360,  0.0615,  ..., -0.0089, -0.0587,  0.1323],\n",
       "        [-0.0310,  0.0188,  0.1457,  ...,  0.0583, -0.0184,  0.0235],\n",
       "        [-0.1548, -0.0597, -0.1306,  ...,  0.0993, -0.0981, -0.1199],\n",
       "        ...,\n",
       "        [-0.0158, -0.0544, -0.0908,  ...,  0.0270,  0.2075,  0.1604],\n",
       "        [ 0.0695, -0.0129,  0.0099,  ...,  0.0773,  0.0125, -0.0410],\n",
       "        [ 0.0142, -0.1359, -0.0463,  ..., -0.0331, -0.1123, -0.1860]])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reordered_representations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "8490b870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reordered_representations[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "361e607a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing lanczos interpolation with cutoff=0.500 and 3 lobes.\n",
      "Interpolated LLM representations to match fMRI TRs via Lanczos downsampling.\n"
     ]
    }
   ],
   "source": [
    "interpolated_representations = lanczosinterp2D(\n",
    "    reordered_representations.to(\"cpu\"),             # shape: (n_samples_input, n_dim)\n",
    "    word_times,      # shape: (n_samples_input,)\n",
    "    fmri_time,     # shape: (n_samples_target,)\n",
    "    window=3,         # (optional) number of lobes for the window\n",
    "    cutoff_mult=1.0,  # (optional) cutoff frequency multiplier\n",
    "    rectify=False     # (optional) rectification\n",
    ")\n",
    "print(\"Interpolated LLM representations to match fMRI TRs via Lanczos downsampling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "470c3e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1351, 2048)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolated_representations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4b881",
   "metadata": {},
   "source": [
    "## 4. Concatenate last x TRs of the LLM representation\n",
    "No need to filter the first x features because we skip those anyway in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "225ef5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_lag = 4  # number of previous TRs to concatenate\n",
    "interpolated_representations = concat_past_features(torch.from_numpy(interpolated_representations), N_lag).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "1df19dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1351, 10240)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolated_representations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00be8447",
   "metadata": {},
   "source": [
    "## 5. Filter out TRs at the boundary of multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "6a26b4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted edges from the LLM representations to match the filtering of fMRI runs (removed first 20 and last 15 of every run).\n"
     ]
    }
   ],
   "source": [
    "# Delete the first 2 and last 1 elements from each experiment run\n",
    "n_begin = 20\n",
    "n_last = 15\n",
    "mask = fmri_runs # This is an integer mask indicating which TRs belong to which run.\n",
    "final_representations = delete_block_edges(interpolated_representations, \n",
    "                                           mask, \n",
    "                                           n_begin, \n",
    "                                           n_last, \n",
    "                                           axis=0) # axis 0: time-dimension is the first in our data\n",
    "print(f\"Deleted edges from the LLM representations to match the filtering of fMRI runs (removed first {n_begin} and last {n_last} of every run).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "f091cbad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1211, 25263)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmri_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "8899a8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1211, 10240)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_representations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84666e1f",
   "metadata": {},
   "source": [
    "## 6. Run the nested blocked cross-validation to find the best alpha per voxel for ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "8b794f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to torch tensors and move to GPU\n",
    "X = torch.tensor(final_representations, dtype=torch.float32, device=\"cpu\") # shape (n_TRs, n_features)\n",
    "y = torch.tensor(fmri_data, dtype=torch.float32, device=\"cpu\") # shape (n_TRs, n_voxels)\n",
    "\n",
    "# Set parameters for crossvalidation\n",
    "split_function = \"blocked\" # divide data into uniform folds\n",
    "block_labels = None  # Not needed for blocked splitting - useful for experiment-wise CV folds\n",
    "n_splits_outer = 4   # Four blocks for outer CV (must be larger than 1)\n",
    "n_splits_inner = 3   # Three blocks for inner CV (must be larger than 1)\n",
    "gap = 15 # number of TRs to skip/discard in between train and test to avoid leakage\n",
    "alphas = [0.1,1,10,100,1000,10000] # default vals from https://github.com/mtoneva/brain_language_nlp/blob/master/utils/utils.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "0b0f8642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1211, 10240]), torch.Size([1211, 25263]))"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "1de37620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting nested blocked cross-validation to find the best alpha per voxel for ridge regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Ridge Regression models fitted:   3%|▎         | 3/96 [00:29<15:07,  9.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([894, 25263]) y_train_outer shape\n",
      "(25263,) best alphas for fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:   6%|▋         | 6/96 [00:57<14:01,  9.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([894, 25263]) y_train_outer shape\n",
      "(25263,) best alphas for fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:   9%|▉         | 9/96 [01:25<13:29,  9.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([894, 25263]) y_train_outer shape\n",
      "(25263,) best alphas for fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:  12%|█▎        | 12/96 [01:52<12:58,  9.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([894, 25263]) y_train_outer shape\n",
      "(25263,) best alphas for fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:  16%|█▌        | 15/96 [02:20<12:30,  9.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([894, 25263]) y_train_outer shape\n",
      "(25263,) best alphas for fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:  19%|█▉        | 18/96 [02:48<11:59,  9.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([894, 25263]) y_train_outer shape\n",
      "(25263,) best alphas for fold\n",
      "Best alphas for fold: 25263 0 voxels with no best alpha found\n",
      "(1, 25263) best alphas shape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:  20%|█▉        | 19/96 [04:32<48:37, 37.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer scores for this fold: [-0.0332336  -0.07788308 -0.11560593 ...  0.00968779  0.05811946\n",
      "  0.01745196]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:  23%|██▎       | 22/96 [05:03<24:13, 19.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([879, 25263]) y_train_outer shape\n",
      "(25263,) best alphas for fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:  26%|██▌       | 25/96 [05:31<15:16, 12.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([879, 25263]) y_train_outer shape\n",
      "(25263,) best alphas for fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:  29%|██▉       | 28/96 [05:59<12:00, 10.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([879, 25263]) y_train_outer shape\n",
      "(25263,) best alphas for fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:  32%|███▏      | 31/96 [06:28<10:37,  9.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([879, 25263]) y_train_outer shape\n",
      "(25263,) best alphas for fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:  35%|███▌      | 34/96 [06:56<09:58,  9.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([879, 25263]) y_train_outer shape\n",
      "(25263,) best alphas for fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:  39%|███▊      | 37/96 [07:24<09:19,  9.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([879, 25263]) y_train_outer shape\n",
      "(25263,) best alphas for fold\n",
      "Best alphas for fold: 25263 0 voxels with no best alpha found\n",
      "(2, 25263) best alphas shape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:  40%|███▉      | 38/96 [09:08<36:20, 37.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer scores for this fold: [ 0.05543184  0.08387576 -0.00931988 ...  0.01935829  0.05239794\n",
      "  0.04088382]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:  43%|████▎     | 41/96 [09:37<17:44, 19.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([879, 25263]) y_train_outer shape\n",
      "(25263,) best alphas for fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:  46%|████▌     | 44/96 [10:05<11:04, 12.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([879, 25263]) y_train_outer shape\n",
      "(25263,) best alphas for fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:  49%|████▉     | 47/96 [10:33<08:37, 10.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([879, 25263]) y_train_outer shape\n",
      "(25263,) best alphas for fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:  52%|█████▏    | 50/96 [11:02<07:28,  9.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([879, 25263]) y_train_outer shape\n",
      "(25263,) best alphas for fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:  55%|█████▌    | 53/96 [11:30<06:48,  9.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([879, 25263]) y_train_outer shape\n",
      "(25263,) best alphas for fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:  58%|█████▊    | 56/96 [11:58<06:16,  9.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([879, 25263]) y_train_outer shape\n",
      "(25263,) best alphas for fold\n",
      "Best alphas for fold: 25263 0 voxels with no best alpha found\n",
      "(3, 25263) best alphas shape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:  59%|█████▉    | 57/96 [13:31<22:29, 34.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer scores for this fold: [0.09816635 0.12470783 0.09702691 ... 0.02867154 0.08311213 0.10760191]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:  62%|██████▎   | 60/96 [14:00<10:57, 18.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([891, 25263]) y_train_outer shape\n",
      "(25263,) best alphas for fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:  66%|██████▌   | 63/96 [14:29<06:50, 12.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([891, 25263]) y_train_outer shape\n",
      "(25263,) best alphas for fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:  69%|██████▉   | 66/96 [14:57<05:14, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([891, 25263]) y_train_outer shape\n",
      "(25263,) best alphas for fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ridge Regression models fitted:  71%|███████   | 68/96 [15:16<04:39,  9.98s/it]"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "print(\"Starting nested blocked cross-validation to find the best alpha per voxel for ridge regression\")\n",
    "# Obtain the best ridge regression penalties for each voxel independently (~25k alphas)\n",
    "best_alphas, outer_scores = nested_blocked_cv(\n",
    "    X, y,\n",
    "    split_function=split_function,\n",
    "    block_labels=block_labels,\n",
    "    n_splits_outer=n_splits_outer,\n",
    "    n_splits_inner=n_splits_inner,\n",
    "    gap=gap,\n",
    "    alphas=alphas,\n",
    "    device=device\n",
    ")\n",
    "print(\"Best alphas found:\", best_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_labels = [[0 for _ in range(X.shape[0])]] # everything comes from the same story so we assume identical block_labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f391a772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9db78f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cajal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
